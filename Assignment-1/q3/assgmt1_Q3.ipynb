{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# PART I: Running a SpeechBrain ASR Recipe"
      ],
      "metadata": {
        "id": "p3S0Etz_Nokz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the codebase."
      ],
      "metadata": {
        "id": "bKXIYTJSOnt_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "A5H2lpHX7npZ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Clone SpeechBrain repository\n",
        "!git clone https://github.com/Darshan7575/speechbrain.git\n",
        "%cd /content/speechbrain/\n",
        "\n",
        "# Install required dependencies\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Install SpeechBrain in editable mode\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import logging\n",
        "import sys\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "from speechbrain.utils.data_utils import get_all_files, download_file\n",
        "from speechbrain.dataio.dataio import read_audio\n",
        "from speechbrain.utils.distributed import run_on_main, if_main_process\n",
        "\n",
        "# Required variables and loggers\n",
        "logger = logging.getLogger(__name__)\n",
        "logger = logging.getLogger(__name__)\n",
        "MINILIBRI_TRAIN_URL = \"http://www.openslr.org/resources/31/train-clean-5.tar.gz\"\n",
        "MINILIBRI_VALID_URL = \"http://www.openslr.org/resources/31/dev-clean-2.tar.gz\"\n",
        "MINILIBRI_TEST_URL = \"https://www.openslr.org/resources/12/test-clean.tar.gz\"\n",
        "SAMPLERATE = 16000\n",
        "\n",
        "device=\"cuda\"\n",
        "run_opts = {'device':device}"
      ],
      "metadata": {
        "id": "61C_gxJo5aRH"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer Training\n",
        "In this section, we will train a BPE tokenizer with **150 tokens** using `Sentencepiece`.\n",
        "\n"
      ],
      "metadata": {
        "id": "rtdr1VnyCQTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ############################################################################\n",
        "# Dataset creation helper functions\n",
        "# ############################################################################\n",
        "\n",
        "def prepare_mini_librispeech(\n",
        "    data_folder, save_json_train, save_json_valid, save_json_test\n",
        "):\n",
        "    \"\"\"\n",
        "    Prepares the json files for the Mini Librispeech dataset.\n",
        "    Downloads the dataset if its not found in the `data_folder`.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if this phase is already done (if so, skip it)\n",
        "    if skip(save_json_train, save_json_valid, save_json_test):\n",
        "        logger.info(\"Preparation completed in previous run, skipping.\")\n",
        "        return\n",
        "\n",
        "    # If the dataset doesn't exist yet, download it\n",
        "    train_folder = os.path.join(data_folder, \"LibriSpeech\", \"train-clean-5\")\n",
        "    valid_folder = os.path.join(data_folder, \"LibriSpeech\", \"dev-clean-2\")\n",
        "    test_folder = os.path.join(data_folder, \"LibriSpeech\", \"test-clean\")\n",
        "    if not check_folders(train_folder, valid_folder, test_folder):\n",
        "        download_mini_librispeech(data_folder)\n",
        "\n",
        "    # List files and create manifest from list\n",
        "    logger.info(\n",
        "        f\"Creating {save_json_train}, {save_json_valid}, and {save_json_test}\"\n",
        "    )\n",
        "    extension = [\".flac\"]\n",
        "\n",
        "    # List of flac audio files\n",
        "    wav_list_train = get_all_files(train_folder, match_and=extension)\n",
        "    wav_list_valid = get_all_files(valid_folder, match_and=extension)\n",
        "    wav_list_test = get_all_files(test_folder, match_and=extension)\n",
        "\n",
        "    # List of transcription file\n",
        "    extension = [\".trans.txt\"]\n",
        "    trans_list = get_all_files(data_folder, match_and=extension)\n",
        "    trans_dict = get_transcription(trans_list)\n",
        "\n",
        "    # Create the json files\n",
        "    create_json(wav_list_train, trans_dict, save_json_train)\n",
        "    create_json(wav_list_valid, trans_dict, save_json_valid)\n",
        "    create_json(wav_list_test, trans_dict, save_json_test)\n",
        "\n",
        "\n",
        "def get_transcription(trans_list):\n",
        "    \"\"\"\n",
        "    Returns a dictionary with the transcription of each sentence in the dataset.\n",
        "    \"\"\"\n",
        "    # Processing all the transcription files in the list\n",
        "    trans_dict = {}\n",
        "    for trans_file in trans_list:\n",
        "        # Reading the text file\n",
        "        with open(trans_file) as f:\n",
        "            for line in f:\n",
        "                uttid = line.split(\" \")[0]\n",
        "                text = line.rstrip().split(\" \")[1:]\n",
        "                text = \" \".join(text)\n",
        "                trans_dict[uttid] = text\n",
        "\n",
        "    logger.info(\"Transcription files read!\")\n",
        "    return trans_dict\n",
        "\n",
        "\n",
        "def create_json(wav_list, trans_dict, json_file):\n",
        "    \"\"\"\n",
        "    Creates the json file given a list of wav files and their transcriptions.\n",
        "    \"\"\"\n",
        "    # Processing all the wav files in the list\n",
        "    json_dict = {}\n",
        "    for wav_file in wav_list:\n",
        "\n",
        "        # Reading the signal (to retrieve duration in seconds)\n",
        "        signal = read_audio(wav_file)\n",
        "        duration = signal.shape[0] / SAMPLERATE\n",
        "\n",
        "        # Manipulate path to get relative path and uttid\n",
        "        path_parts = wav_file.split(os.path.sep)\n",
        "        uttid, _ = os.path.splitext(path_parts[-1])\n",
        "        relative_path = os.path.join(\"{data_root}\", *path_parts[-5:])\n",
        "\n",
        "        # Create entry for this utterance\n",
        "        json_dict[uttid] = {\n",
        "            \"wav\": relative_path,\n",
        "            \"length\": duration,\n",
        "            \"words\": trans_dict[uttid],\n",
        "        }\n",
        "\n",
        "    # Writing the dictionary to the json file\n",
        "    with open(json_file, mode=\"w\") as json_f:\n",
        "        json.dump(json_dict, json_f, indent=2)\n",
        "\n",
        "    logger.info(f\"{json_file} successfully created!\")\n",
        "\n",
        "\n",
        "def skip(*filenames):\n",
        "    \"\"\"\n",
        "    Detects if the data preparation has been already done.\n",
        "    If the preparation has been done, we can skip it.\n",
        "    \"\"\"\n",
        "    for filename in filenames:\n",
        "        if not os.path.isfile(filename):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def check_folders(*folders):\n",
        "    \"\"\"Returns False if any passed folder does not exist.\"\"\"\n",
        "    for folder in folders:\n",
        "        if not os.path.exists(folder):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def download_mini_librispeech(destination):\n",
        "    \"\"\"Download dataset and unpack it.\n",
        "    \"\"\"\n",
        "    train_archive = os.path.join(destination, \"train-clean-5.tar.gz\")\n",
        "    valid_archive = os.path.join(destination, \"dev-clean-2.tar.gz\")\n",
        "    test_archive = os.path.join(destination, \"test-clean.tar.gz\")\n",
        "    download_file(MINILIBRI_TRAIN_URL, train_archive)\n",
        "    download_file(MINILIBRI_VALID_URL, valid_archive)\n",
        "    download_file(MINILIBRI_TEST_URL, test_archive)\n",
        "    shutil.unpack_archive(train_archive, destination)\n",
        "    shutil.unpack_archive(valid_archive, destination)\n",
        "    shutil.unpack_archive(test_archive, destination)"
      ],
      "metadata": {
        "id": "ujToJHWC4T5y"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_hyperparams = \"\"\"\n",
        "# ############################################################################\n",
        "# Tokenizer: subword BPE with unigram 150\n",
        "# ############################################################################\n",
        "\n",
        "output_folder: !ref results/tokenizer/\n",
        "\n",
        "# Data files\n",
        "data_folder: data\n",
        "train_annotation: !ref <data_folder>/train.json\n",
        "valid_annotation: !ref <data_folder>/valid.json\n",
        "test_annotation: !ref <data_folder>/test.json\n",
        "\n",
        "# Tokenizer training parameters\n",
        "token_type: unigram  # [\"unigram\", \"bpe\", \"char\"]\n",
        "token_output: 150  # index(blank/eos/bos/unk) = 0\n",
        "character_coverage: 1.0\n",
        "json_read: words\n",
        "\n",
        "tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece\n",
        "   model_dir: !ref <output_folder>\n",
        "   vocab_size: !ref <token_output>\n",
        "   annotation_train: !ref <train_annotation>\n",
        "   annotation_read: !ref <json_read>\n",
        "   annotation_format: json\n",
        "   model_type: !ref <token_type> # [\"unigram\", \"bpe\", \"char\"]\n",
        "   character_coverage: !ref <character_coverage>\n",
        "   annotation_list_to_check: [!ref <train_annotation>, !ref <valid_annotation>]\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rz9pyHan4V0S"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load required params from the hyperpyyaml file\n",
        "hparams = load_hyperpyyaml(tokenizer_hyperparams)\n",
        "\n",
        "# 1. Dataset creation\n",
        "\n",
        "## Create experiment directory\n",
        "sb.create_experiment_directory(\n",
        "    experiment_directory=hparams[\"output_folder\"],\n",
        "    overrides=None,\n",
        ")\n",
        "\n",
        "## Create dataset\n",
        "run_on_main(\n",
        "    prepare_mini_librispeech,\n",
        "    kwargs={\n",
        "        \"data_folder\": hparams[\"data_folder\"],\n",
        "        \"save_json_train\": hparams[\"train_annotation\"],\n",
        "        \"save_json_valid\": hparams[\"valid_annotation\"],\n",
        "        \"save_json_test\": hparams[\"test_annotation\"],\n",
        "    },\n",
        ")\n",
        "\n",
        "# 2. Tokenizer training\n",
        "hparams[\"tokenizer\"]()\n",
        "\n",
        "# 3. Saving tokenizer in .ckpt extension\n",
        "output_path = hparams[\"output_folder\"]\n",
        "token_output = hparams[\"token_output\"]\n",
        "token_type = hparams[\"token_type\"]\n",
        "bpe_model = f\"{output_path}/{token_output}_{token_type}.model\"\n",
        "tokenizer_ckpt = f\"{output_path}/tokenizer.ckpt\"\n",
        "shutil.copyfile(bpe_model, tokenizer_ckpt)"
      ],
      "metadata": {
        "id": "nA8xD6Do4Xl7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "52aa0892-fd0f-4ea7-a981-8a835d6c052f"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/tokenizer/\n",
            "__main__ - Preparation completed in previous run, skipping.\n",
            "speechbrain.tokenizers.SentencePiece - Tokenizer is already trained.\n",
            "speechbrain.tokenizers.SentencePiece - ==== Loading Tokenizer ===\n",
            "speechbrain.tokenizers.SentencePiece - Tokenizer path: results/tokenizer/150_unigram.model\n",
            "speechbrain.tokenizers.SentencePiece - Tokenizer vocab_size: 150\n",
            "speechbrain.tokenizers.SentencePiece - Tokenizer type: unigram\n",
            "speechbrain.tokenizers.SentencePiece - ==== Accuracy checking for recovering text from tokenizer ===\n",
            "speechbrain.tokenizers.SentencePiece - recover words from: data/train.json\n",
            "speechbrain.tokenizers.SentencePiece - Wrong recover words: 0\n",
            "speechbrain.tokenizers.SentencePiece - accuracy recovering words: 1.0\n",
            "speechbrain.tokenizers.SentencePiece - ==== Accuracy checking for recovering text from tokenizer ===\n",
            "speechbrain.tokenizers.SentencePiece - recover words from: data/valid.json\n",
            "speechbrain.tokenizers.SentencePiece - Wrong recover words: 0\n",
            "speechbrain.tokenizers.SentencePiece - accuracy recovering words: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'results/tokenizer//tokenizer.ckpt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training\n",
        "In this section, we will train a **6 layer Conformer** encoder only architecture with the `CTC objective`."
      ],
      "metadata": {
        "id": "vwqFx3QcOdd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_hyperparams = \"\"\"\n",
        "# Seed needs to be set at top of yaml, before objects with parameters are made\n",
        "seed: 2024\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Data files\n",
        "data_folder: data\n",
        "train_annotation: !ref <data_folder>/train.json\n",
        "valid_annotation: !ref <data_folder>/valid.json\n",
        "test_annotation: !ref <data_folder>/test.json\n",
        "\n",
        "# Language model (LM) pretraining\n",
        "pretrained_lm_tokenizer_path: ./results/tokenizer\n",
        "\n",
        "# Training parameters\n",
        "number_of_epochs: 30\n",
        "batch_size: 8\n",
        "lr_adam: 0.001\n",
        "max_grad_norm: 5.0\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "loss_reduction: 'batchmean'\n",
        "\n",
        "# Dataloader options\n",
        "train_dataloader_opts:\n",
        "    batch_size: !ref <batch_size>\n",
        "\n",
        "valid_dataloader_opts:\n",
        "    batch_size: !ref <batch_size>\n",
        "\n",
        "test_dataloader_opts:\n",
        "    batch_size: !ref <batch_size>\n",
        "\n",
        "# Feature parameters\n",
        "sample_rate: 16000\n",
        "n_fft: 400\n",
        "n_mels: 80\n",
        "\n",
        "####################### Model parameters ###########################\n",
        "# Transformer\n",
        "d_model: 128\n",
        "nhead: 4\n",
        "num_encoder_layers: 6\n",
        "d_ffn: 512\n",
        "transformer_dropout: 0.1\n",
        "activation: !name:torch.nn.GELU\n",
        "output_neurons: 150\n",
        "label_smoothing: 0.0\n",
        "attention_type: RelPosMHAXL\n",
        "\n",
        "# Outputs\n",
        "blank_index: 0\n",
        "pad_index: 0\n",
        "bos_index: 1\n",
        "eos_index: 2\n",
        "\n",
        "# Decoding parameters\n",
        "min_decode_ratio: 0.0\n",
        "max_decode_ratio: 1.0\n",
        "test_beam_size: 1\n",
        "ctc_weight_decode: 1.0\n",
        "\n",
        "############################## models ################################\n",
        "\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    sample_rate: !ref <sample_rate>\n",
        "    n_fft: !ref <n_fft>\n",
        "    n_mels: !ref <n_mels>\n",
        "\n",
        "CNN: !new:speechbrain.lobes.models.convolution.ConvolutionFrontEnd\n",
        "    input_shape: (8, 10, 80)\n",
        "    num_blocks: 2\n",
        "    num_layers_per_block: 1\n",
        "    out_channels: (64, 32)\n",
        "    kernel_sizes: (3, 3)\n",
        "    strides: (2, 2)\n",
        "    residuals: (False, False)\n",
        "\n",
        "# standard parameters for the BASE model\n",
        "Transformer: !new:speechbrain.lobes.models.transformer.TransformerASR.TransformerASR\n",
        "    input_size: 640\n",
        "    tgt_vocab: !ref <output_neurons>\n",
        "    d_model: !ref <d_model>\n",
        "    nhead: !ref <nhead>\n",
        "    num_encoder_layers: !ref <num_encoder_layers>\n",
        "    num_decoder_layers: 0\n",
        "    d_ffn: !ref <d_ffn>\n",
        "    dropout: !ref <transformer_dropout>\n",
        "    activation: !ref <activation>\n",
        "    encoder_module: conformer\n",
        "    attention_type: !ref <attention_type>\n",
        "    normalize_before: True\n",
        "\n",
        "tokenizer: !new:sentencepiece.SentencePieceProcessor\n",
        "\n",
        "ctc_lin: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <d_model>\n",
        "    n_neurons: !ref <output_neurons>\n",
        "\n",
        "normalize: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: global\n",
        "    update_until_epoch: 4\n",
        "\n",
        "modules:\n",
        "    CNN: !ref <CNN>\n",
        "    Transformer: !ref <Transformer>\n",
        "    ctc_lin: !ref <ctc_lin>\n",
        "    normalize: !ref <normalize>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <CNN>, !ref <Transformer>, !ref <ctc_lin>]\n",
        "\n",
        "# define two optimizers here for two-stage training\n",
        "Adam: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_adam>\n",
        "    betas: (0.9, 0.98)\n",
        "    eps: 0.000000001\n",
        "\n",
        "log_softmax: !new:torch.nn.LogSoftmax\n",
        "    dim: -1\n",
        "\n",
        "ctc_cost: !name:speechbrain.nnet.losses.ctc_loss\n",
        "    blank_index: !ref <blank_index>\n",
        "    reduction: !ref <loss_reduction>\n",
        "\n",
        "noam_annealing: !new:speechbrain.nnet.schedulers.NoamScheduler\n",
        "    lr_initial: !ref <lr_adam>\n",
        "    n_warmup_steps: 1500\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "error_rate_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats\n",
        "\n",
        "cer_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats\n",
        "   split_tokens: True\n",
        "\n",
        "# The pretrainer allows a mapping between pretrained files and instances that\n",
        "# are declared in the yaml. E.g here, we will download the file tokenizer.ckpt\n",
        "# and it will be loaded into \"tokenizer\" which is pointing to the <pretrained_lm_tokenizer_path> defined\n",
        "# before.\n",
        "pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer\n",
        "    loadables:\n",
        "        tokenizer: !ref <tokenizer>\n",
        "    paths:\n",
        "        tokenizer: !ref <pretrained_lm_tokenizer_path>/tokenizer.ckpt\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "6WPcwXueCRm7"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataio_prepare(hparams):\n",
        "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
        "    It also defines the data processing pipeline through user-defined functions.\n",
        "    \"\"\"\n",
        "    # Define audio pipeline. In this case, we simply read the path contained\n",
        "    # in the variable wav with the audio reader.\n",
        "    @sb.utils.data_pipeline.takes(\"wav\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav):\n",
        "        \"\"\"Load the audio signal. This is done on the CPU in the `collate_fn`.\"\"\"\n",
        "        sig = sb.dataio.dataio.read_audio(wav)\n",
        "        return sig\n",
        "\n",
        "    tokenizer = hparams[\"tokenizer\"]\n",
        "    # Define text processing pipeline. We start from the raw text and then\n",
        "    # encode it using the tokenizer. The tokens with BOS are used for feeding\n",
        "    # decoder during training, the tokens with EOS for computing the cost function.\n",
        "    # The tokens without BOS or EOS is for computing CTC loss.\n",
        "    @sb.utils.data_pipeline.takes(\"words\")\n",
        "    @sb.utils.data_pipeline.provides(\n",
        "        \"wrd\", \"tokens_list\", \"tokens_bos\", \"tokens_eos\", \"tokens\"\n",
        "    )\n",
        "    def text_pipeline(wrd):\n",
        "        \"\"\"Processes the transcriptions to generate proper labels\"\"\"\n",
        "        yield wrd\n",
        "        tokens_list = tokenizer.encode_as_ids(wrd)\n",
        "        yield tokens_list\n",
        "        tokens_bos = torch.LongTensor([hparams[\"bos_index\"]] + (tokens_list))\n",
        "        yield tokens_bos\n",
        "        tokens_eos = torch.LongTensor(tokens_list + [hparams[\"eos_index\"]])\n",
        "        yield tokens_eos\n",
        "        tokens = torch.LongTensor(tokens_list)\n",
        "        yield tokens\n",
        "\n",
        "    # Define datasets from json data manifest file\n",
        "    # Define datasets sorted by ascending lengths for efficiency\n",
        "    datasets = {}\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "    for dataset in [\"train\", \"valid\", \"test\"]:\n",
        "        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
        "            json_path=hparams[f\"{dataset}_annotation\"],\n",
        "            replacements={\"data_root\": data_folder},\n",
        "            dynamic_items=[audio_pipeline, text_pipeline],\n",
        "            output_keys=[\n",
        "                \"id\",\n",
        "                \"sig\",\n",
        "                \"wrd\",\n",
        "                \"tokens_bos\",\n",
        "                \"tokens_eos\",\n",
        "                \"tokens\",\n",
        "            ],\n",
        "        )\n",
        "        hparams[f\"{dataset}_dataloader_opts\"][\"shuffle\"] = False\n",
        "\n",
        "    datasets[\"train\"] = datasets[\"train\"].filtered_sorted(sort_key=\"length\")\n",
        "    hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
        "\n",
        "    return (\n",
        "        datasets[\"train\"],\n",
        "        datasets[\"valid\"],\n",
        "        datasets[\"test\"],\n",
        "        tokenizer\n",
        "    )"
      ],
      "metadata": {
        "id": "euJMqDLSWv7Y"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training procedure\n",
        "class BaseASR(sb.Brain):\n",
        "    def __init__(\n",
        "        self,\n",
        "        modules=None,\n",
        "        opt_class=None,\n",
        "        hparams=None,\n",
        "        run_opts=None,\n",
        "        checkpointer=None,\n",
        "        profiler=None,\n",
        "        tokenizer=None,\n",
        "    ):\n",
        "        super(BaseASR, self).__init__(\n",
        "            modules=modules,\n",
        "            opt_class=opt_class,\n",
        "            hparams=hparams,\n",
        "            run_opts=run_opts,\n",
        "            checkpointer=checkpointer,\n",
        "            profiler=profiler\n",
        "        )\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Performs a forward pass through the encoder\"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, wav_lens = batch.sig\n",
        "        tokens_bos, _ = batch.tokens_bos\n",
        "\n",
        "        # compute features\n",
        "        feats = self.hparams.compute_features(wavs)#### FILL THIS WITH YOUR CODE FROM PART I ####\n",
        "        current_epoch = self.hparams.epoch_counter.current\n",
        "        feats = self.modules.normalize(feats, wav_lens, epoch=current_epoch)\n",
        "\n",
        "        # forward modules\n",
        "        src = self.modules.CNN(feats)\n",
        "\n",
        "        enc_out, _ = self.modules.Transformer(\n",
        "            src, tokens_bos, wav_lens, pad_idx=self.hparams.pad_index,\n",
        "        )\n",
        "\n",
        "        # output layer for ctc log-probabilities\n",
        "        logits = self.modules.ctc_lin(enc_out)\n",
        "        loss_func = sb.nnet.activations.Softmax(apply_log=True)\n",
        "        p_ctc = loss_func.forward(logits)######\n",
        "\n",
        "        # Compute outputs\n",
        "        hyps = None\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            hyps = None\n",
        "        else:\n",
        "            hyps = sb.decoders.ctc_greedy_decode(\n",
        "                p_ctc, wav_lens, blank_id=self.hparams.blank_index\n",
        "            )\n",
        "\n",
        "        return p_ctc, wav_lens, hyps\n",
        "\n",
        "    def get_ctc_probs(self, batch):\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, wav_lens = batch.sig\n",
        "        tokens_bos, _ = batch.tokens_bos\n",
        "\n",
        "        # compute features\n",
        "        feats = self.hparams.compute_features(wavs)\n",
        "        current_epoch = self.hparams.epoch_counter.current\n",
        "        feats = self.modules.normalize(feats, wav_lens, epoch=current_epoch)\n",
        "\n",
        "        # forward modules\n",
        "        src = self.modules.CNN(feats)\n",
        "\n",
        "        enc_out, _ = self.modules.Transformer(\n",
        "            src, tokens_bos, wav_lens, pad_idx=self.hparams.pad_index,\n",
        "        )\n",
        "\n",
        "        logits = self.modules.ctc_lin(enc_out)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the CTC loss given predictions and targets.\"\"\"\n",
        "\n",
        "        (p_ctc, wav_lens, hyps,) = predictions\n",
        "\n",
        "        ids = batch.id\n",
        "        tokens_eos, tokens_eos_lens = batch.tokens_eos\n",
        "        tokens, tokens_lens = batch.tokens\n",
        "\n",
        "        # Calculate CTC loss\n",
        "         #### FILL THIS WITH YOUR CODE FROM PART I ####\n",
        "        loss = sb.nnet.losses.ctc_loss(\n",
        "        p_ctc, tokens, wav_lens, tokens_lens, blank_index=self.hparams.blank_index\n",
        "    )  ######\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            # Decode token terms to words\n",
        "            predicted_words = [\n",
        "                self.tokenizer.decode_ids(utt_seq).split(\" \") for utt_seq in hyps\n",
        "            ]\n",
        "            target_words = [wrd.split(\" \") for wrd in batch.wrd]\n",
        "            self.wer_metric.append(ids, predicted_words, target_words)\n",
        "            self.cer_metric.append(ids, predicted_words, target_words)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_evaluate_start(self, max_key=None, min_key=None):\n",
        "        \"\"\"Performs checkpoint averge if needed\"\"\"\n",
        "        super().on_evaluate_start()\n",
        "\n",
        "        ckpts = self.checkpointer.find_checkpoints(\n",
        "            max_key=max_key, min_key=min_key\n",
        "        )\n",
        "        ckpt = sb.utils.checkpoints.average_checkpoints(\n",
        "            ckpts, recoverable_name=\"model\", device=self.device\n",
        "        )\n",
        "\n",
        "        self.hparams.model.load_state_dict(ckpt, strict=True)\n",
        "        self.hparams.model.eval()\n",
        "        print(\"Loaded the average\")\n",
        "\n",
        "    def evaluate_batch(self, batch, stage):\n",
        "        \"\"\"Computations needed for validation/test batches\"\"\"\n",
        "        with torch.no_grad():\n",
        "            predictions = self.compute_forward(batch, stage=stage)\n",
        "            loss = self.compute_objectives(predictions, batch, stage=stage)\n",
        "        return loss.detach()\n",
        "\n",
        "    def on_stage_start(self, stage, epoch):\n",
        "        \"\"\"Gets called at the beginning of each epoch\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.cer_metric = self.hparams.cer_computer()\n",
        "            self.wer_metric = self.hparams.error_rate_computer()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch):\n",
        "        \"\"\"Gets called at the end of a epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"CER\"] = self.cer_metric.summarize(\"error_rate\")\n",
        "            stage_stats[\"WER\"] = self.wer_metric.summarize(\"error_rate\")\n",
        "\n",
        "        # log stats and save checkpoint at end-of-epoch\n",
        "        if stage == sb.Stage.VALID and sb.utils.distributed.if_main_process():\n",
        "\n",
        "            lr = self.hparams.noam_annealing.current_lr\n",
        "            steps = self.optimizer_step\n",
        "            optimizer = self.optimizer.__class__.__name__\n",
        "\n",
        "            epoch_stats = {\n",
        "                \"epoch\": epoch,\n",
        "                \"lr\": lr,\n",
        "                \"steps\": steps,\n",
        "                \"optimizer\": optimizer,\n",
        "            }\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta=epoch_stats,\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            # Save only last 10 checkpoints\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"loss\": stage_loss, \"epoch\": epoch},\n",
        "                max_keys=[\"epoch\"],\n",
        "                num_to_keep=10,\n",
        "            )\n",
        "\n",
        "        elif stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "            # Write the WER metric for test dataset\n",
        "            if if_main_process():\n",
        "                with open(self.hparams.test_wer_file, \"w\") as w:\n",
        "                    self.wer_metric.write_stats(w)\n",
        "\n",
        "    def fit_batch(self, batch):\n",
        "        \"\"\"Performs a forward + backward pass on 1 batch\n",
        "        \"\"\"\n",
        "\n",
        "        should_step = self.step % self.grad_accumulation_factor == 0\n",
        "\n",
        "        outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
        "        loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
        "        loss.backward()\n",
        "        if self.check_gradients(loss):\n",
        "            self.optimizer.step()\n",
        "        self.zero_grad()\n",
        "        self.optimizer_step += 1\n",
        "        self.hparams.noam_annealing(self.optimizer)\n",
        "\n",
        "        self.on_fit_batch_end(batch, outputs, loss, should_step)\n",
        "        return loss.detach().cpu()"
      ],
      "metadata": {
        "id": "wUEBBUOQZ4V-"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_hyperparameters = \"\"\"\n",
        "# Setup the directory to host experiment results\n",
        "output_folder: !ref results/transformer/Task_1\n",
        "wer_file: !ref <output_folder>/wer.txt\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        noam_scheduler: !ref <noam_annealing>\n",
        "        normalizer: !ref <normalize>\n",
        "        counter: !ref <epoch_counter>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DDQEQ8M2MNAi"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparams = global_hyperparams + task_hyperparameters\n",
        "hparams = load_hyperpyyaml(hyperparams)\n",
        "\n",
        "# Create experiment directory\n",
        "sb.create_experiment_directory(\n",
        "    experiment_directory=hparams[\"output_folder\"],\n",
        "    overrides=None,\n",
        ")\n",
        "\n",
        "# Here we create the datasets objects as well as tokenization and encoding\n",
        "(\n",
        "    train_data,\n",
        "    valid_data,\n",
        "    test_data,\n",
        "    tokenizer\n",
        ") = dataio_prepare(hparams)\n",
        "\n",
        "# We download the pretrained LM from HuggingFace (or elsewhere depending on\n",
        "# the path given in the YAML file). The tokenizer is loaded at the same time.\n",
        "run_on_main(hparams[\"pretrainer\"].collect_files)\n",
        "hparams[\"pretrainer\"].load_collected(device=run_opts[\"device\"])\n",
        "\n",
        "# Trainer initialization\n",
        "asr_brain = BaseASR(\n",
        "    modules=hparams[\"modules\"],\n",
        "    opt_class=hparams[\"Adam\"],\n",
        "    hparams=hparams,\n",
        "    checkpointer=hparams[\"checkpointer\"],\n",
        "    run_opts=run_opts,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# adding objects to trainer:\n",
        "train_dataloader_opts = hparams[\"train_dataloader_opts\"]\n",
        "valid_dataloader_opts = hparams[\"valid_dataloader_opts\"]\n",
        "\n",
        "# Training\n",
        "asr_brain.fit(\n",
        "    asr_brain.hparams.epoch_counter,\n",
        "    train_data,\n",
        "    valid_data,\n",
        "    train_loader_kwargs=train_dataloader_opts,\n",
        "    valid_loader_kwargs=valid_dataloader_opts\n",
        ")\n",
        "\n",
        "# Testing\n",
        "asr_brain.hparams.test_wer_file = asr_brain.hparams.wer_file\n",
        "asr_brain.evaluate(\n",
        "    train_data,\n",
        "    max_key=\"epoch\",\n",
        "    test_loader_kwargs=hparams[\"train_dataloader_opts\"],\n",
        ")"
      ],
      "metadata": {
        "id": "tGonNC7u8hlJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8562d115-4461-46da-88b1-a070bfc5d5b8"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/transformer/Task_1\n",
            "speechbrain.pretrained.fetching - Fetch tokenizer.ckpt: Using existing file/symlink in model_checkpoints/tokenizer.ckpt.\n",
            "speechbrain.utils.parameter_transfer - Set local path in self.paths[tokenizer] = model_checkpoints/tokenizer.ckpt\n",
            "speechbrain.utils.parameter_transfer - Loading pretrained files for: tokenizer\n",
            "speechbrain.utils.parameter_transfer - Redirecting (loading from local path): model_checkpoints/tokenizer.ckpt -> model_checkpoints/tokenizer.ckpt\n",
            "speechbrain.core - Info: max_grad_norm arg from hparam file is used\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - 2.6M trainable parameters in BaseASR\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/transformer/Task_1/save/CKPT+2024-01-31+12-58-28+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:35<00:00,  5.29it/s, train_loss=4.57]\n",
            "100%|██████████| 137/137 [00:08<00:00, 15.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 3, lr: 3.79e-04, steps: 570, optimizer: Adam - train loss: 4.57 - valid loss: 4.61, valid CER: 1.00e+02, valid WER: 1.00e+02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-14-27+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:29<00:00,  6.53it/s, train_loss=4.49]\n",
            "100%|██████████| 137/137 [00:08<00:00, 15.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 4, lr: 5.06e-04, steps: 760, optimizer: Adam - train loss: 4.49 - valid loss: 4.37, valid CER: 99.94, valid WER: 1.00e+02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-15-05+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.64it/s, train_loss=4.01]\n",
            "100%|██████████| 137/137 [00:13<00:00, 10.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 5, lr: 6.33e-04, steps: 950, optimizer: Adam - train loss: 4.01 - valid loss: 3.69, valid CER: 75.07, valid WER: 93.43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-15-48+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.76it/s, train_loss=3.44]\n",
            "100%|██████████| 137/137 [00:13<00:00, 10.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 6, lr: 7.59e-04, steps: 1140, optimizer: Adam - train loss: 3.44 - valid loss: 3.26, valid CER: 67.04, valid WER: 92.10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-16-29+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.60it/s, train_loss=3.04]\n",
            "100%|██████████| 137/137 [00:13<00:00, 10.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 7, lr: 8.86e-04, steps: 1330, optimizer: Adam - train loss: 3.04 - valid loss: 2.96, valid CER: 58.82, valid WER: 89.26\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-17-12+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.epoch_loop - Going into epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.74it/s, train_loss=2.72]\n",
            "100%|██████████| 137/137 [00:14<00:00,  9.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 8, lr: 9.94e-04, steps: 1520, optimizer: Adam - train loss: 2.72 - valid loss: 2.72, valid CER: 53.55, valid WER: 87.13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-17-55+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.69it/s, train_loss=2.45]\n",
            "100%|██████████| 137/137 [00:14<00:00,  9.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 9, lr: 9.37e-04, steps: 1710, optimizer: Adam - train loss: 2.45 - valid loss: 2.54, valid CER: 48.55, valid WER: 84.56\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-18-38+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.epoch_loop - Going into epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:29<00:00,  6.43it/s, train_loss=2.21]\n",
            "100%|██████████| 137/137 [00:14<00:00,  9.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 10, lr: 8.89e-04, steps: 1900, optimizer: Adam - train loss: 2.21 - valid loss: 2.43, valid CER: 46.14, valid WER: 82.28\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-19-23+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.epoch_loop - Going into epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.67it/s, train_loss=2.02]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 11, lr: 8.47e-04, steps: 2090, optimizer: Adam - train loss: 2.02 - valid loss: 2.33, valid CER: 44.34, valid WER: 80.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-20-07+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+12-57-51+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:29<00:00,  6.45it/s, train_loss=1.85]\n",
            "100%|██████████| 137/137 [00:15<00:00,  9.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 12, lr: 8.11e-04, steps: 2280, optimizer: Adam - train loss: 1.85 - valid loss: 2.27, valid CER: 42.48, valid WER: 79.74\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-20-52+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+12-58-28+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.56it/s, train_loss=1.7]\n",
            "100%|██████████| 137/137 [00:15<00:00,  9.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 13, lr: 7.79e-04, steps: 2470, optimizer: Adam - train loss: 1.70 - valid loss: 2.23, valid CER: 41.73, valid WER: 78.70\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-21-36+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-14-27+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.69it/s, train_loss=1.57]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 14, lr: 7.51e-04, steps: 2660, optimizer: Adam - train loss: 1.57 - valid loss: 2.19, valid CER: 40.06, valid WER: 77.76\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-22-21+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-15-05+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.56it/s, train_loss=1.45]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 15, lr: 7.26e-04, steps: 2850, optimizer: Adam - train loss: 1.45 - valid loss: 2.19, valid CER: 39.26, valid WER: 77.36\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-23-05+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-15-48+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:29<00:00,  6.47it/s, train_loss=1.35]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 16, lr: 7.03e-04, steps: 3040, optimizer: Adam - train loss: 1.35 - valid loss: 2.17, valid CER: 38.55, valid WER: 77.02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-23-52+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-16-29+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.58it/s, train_loss=1.25]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 17, lr: 6.82e-04, steps: 3230, optimizer: Adam - train loss: 1.25 - valid loss: 2.17, valid CER: 37.93, valid WER: 76.70\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-24-36+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-17-12+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:29<00:00,  6.49it/s, train_loss=1.17]\n",
            "100%|██████████| 137/137 [00:17<00:00,  8.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 18, lr: 6.62e-04, steps: 3420, optimizer: Adam - train loss: 1.17 - valid loss: 2.17, valid CER: 37.40, valid WER: 76.66\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-25-23+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-17-55+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.71it/s, train_loss=1.09]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 19, lr: 6.45e-04, steps: 3610, optimizer: Adam - train loss: 1.09 - valid loss: 2.21, valid CER: 37.31, valid WER: 76.15\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-26-08+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-18-38+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.74it/s, train_loss=1.01]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 20, lr: 6.28e-04, steps: 3800, optimizer: Adam - train loss: 1.01 - valid loss: 2.23, valid CER: 37.20, valid WER: 75.81\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-26-52+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-19-23+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:29<00:00,  6.43it/s, train_loss=0.955]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 21, lr: 6.13e-04, steps: 3990, optimizer: Adam - train loss: 9.55e-01 - valid loss: 2.22, valid CER: 36.42, valid WER: 75.07\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-27-38+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-20-07+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.74it/s, train_loss=0.891]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 22, lr: 5.99e-04, steps: 4180, optimizer: Adam - train loss: 8.91e-01 - valid loss: 2.27, valid CER: 36.50, valid WER: 74.83\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-28-23+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-20-52+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.71it/s, train_loss=0.83]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 23, lr: 5.86e-04, steps: 4370, optimizer: Adam - train loss: 8.30e-01 - valid loss: 2.25, valid CER: 35.60, valid WER: 73.90\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-29-07+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-21-36+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:29<00:00,  6.46it/s, train_loss=0.784]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 24, lr: 5.74e-04, steps: 4560, optimizer: Adam - train loss: 7.84e-01 - valid loss: 2.27, valid CER: 35.58, valid WER: 73.54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-29-53+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-22-21+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.59it/s, train_loss=0.74]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 25, lr: 5.62e-04, steps: 4750, optimizer: Adam - train loss: 7.40e-01 - valid loss: 2.32, valid CER: 35.59, valid WER: 73.43\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-30-38+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-23-05+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.73it/s, train_loss=0.692]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 26, lr: 5.51e-04, steps: 4940, optimizer: Adam - train loss: 6.92e-01 - valid loss: 2.33, valid CER: 35.23, valid WER: 72.99\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-31-23+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-23-52+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.62it/s, train_loss=0.659]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 27, lr: 5.41e-04, steps: 5130, optimizer: Adam - train loss: 6.59e-01 - valid loss: 2.35, valid CER: 35.02, valid WER: 73.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-32-08+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-24-36+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.65it/s, train_loss=0.615]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 28, lr: 5.31e-04, steps: 5320, optimizer: Adam - train loss: 6.15e-01 - valid loss: 2.37, valid CER: 34.92, valid WER: 72.79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-32-53+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-25-23+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.61it/s, train_loss=0.585]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 29, lr: 5.22e-04, steps: 5510, optimizer: Adam - train loss: 5.85e-01 - valid loss: 2.39, valid CER: 34.71, valid WER: 72.18\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-33-38+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-26-08+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.55it/s, train_loss=0.562]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 30, lr: 5.13e-04, steps: 5700, optimizer: Adam - train loss: 5.62e-01 - valid loss: 2.42, valid CER: 34.92, valid WER: 72.08\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-34-24+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-01-31+14-26-52+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/transformer/Task_1/save/CKPT+2024-01-31+14-34-24+00\n",
            "Loaded the average\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:43<00:00,  4.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - Epoch loaded: 30 - test loss: 3.28e-01, test CER: 7.26, test WER: 24.24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3284426819631143"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART III: Visualizing CTC alignments"
      ],
      "metadata": {
        "id": "JbkMK32DJpHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "(\n",
        "    train_data,\n",
        "    valid_data,\n",
        "    test_data,\n",
        "    _\n",
        ") = dataio_prepare(hparams)\n",
        "\n",
        "#Creating a test dataloader to get model results on test audio\n",
        "hparams['train_dataloader_opts']['batch_size'] = 8\n",
        "test_loader = asr_brain.make_dataloader(test_data, sb.Stage.TEST, **hparams[\"train_dataloader_opts\"])\n",
        "results = None\n",
        "\n",
        "#Creating a batch for the test dataloader and getting model results\n",
        "for batch in test_loader:\n",
        "    ground_truth_tokens = batch.tokens[0]\n",
        "    target_lens = (ground_truth_tokens != 0).sum(-1)\n",
        "    _, input_lens = batch.sig\n",
        "    results = asr_brain.compute_forward(batch, sb.Stage.TEST)[0]\n",
        "    input_lens = torch.round(input_lens * results.shape[1]).to(torch.int)\n",
        "    break\n",
        "\n",
        "print(results.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "zIfWCEVTJvYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50023fff-3762-42ee-c385-3a600092ff3f"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 55])\n",
            "torch.Size([8, 198, 150])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def logadd(x0, x1, x2):\n",
        "\t# produces nan gradients in backward if -inf log-space zero element is used https://github.com/pytorch/pytorch/issues/31829\n",
        "\treturn torch.logsumexp(torch.stack([x0, x1, x2]), dim = 0)"
      ],
      "metadata": {
        "id": "9b4G2mMaBzsV"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ctc_alignment(log_probs : torch.Tensor, targets : torch.Tensor, input_lengths : torch.Tensor, target_lengths : torch.Tensor, blank : int = 0):\n",
        "    '''\n",
        "    this function computes the ctc-alignment matrix, then performs a backward-algorithm\n",
        "    pass on it to find the best state sequence (states being the transformer states here).\n",
        "    the ctc-algorithm equations can be found on the 3rd slide of the following link -\n",
        "    https://www.cse.iitb.ac.in/~pjyothi/cs753/slides/lecture15.pdf\n",
        "    keep in mind that the equations are 1-indexed while our implementation is 0-indexed\n",
        "    '''\n",
        "\n",
        "    max_input_len, batch_size, num_tokens = log_probs.shape\n",
        "    _, max_target_len = targets.shape\n",
        "\n",
        "    batch_arange = torch.arange(batch_size, device = input_lengths.device)\n",
        "\n",
        "    ##############################\n",
        "    # TODO(1) - add \"_\" (blank) token alternatively to target sequence (starting and ending with \"_\")\n",
        "    # example - \"a man saw a cat\" should become \"_a_ _m_a_n_ _s_a_w_ _a_ _c_a_t_\"\n",
        "    # (assuming that each character is one token; a token might be >1 characters long)\n",
        "    # if the original sequence had L tokens, `padded_targets` should now have 2*L+1 tokens\n",
        "   #print(targets)\n",
        "    un_ = 0\n",
        "    b = [ ]\n",
        "    for i in targets:\n",
        "        row=[un_]\n",
        "        for j in range(len(i)):\n",
        "          row.append(i[j])\n",
        "          row.append(un_)\n",
        "        b.append(row)\n",
        "\n",
        "    padded_targets = torch.tensor(b)\n",
        "    ### Complete this\n",
        "    ##############################\n",
        "    assert padded_targets.shape == (batch_size, 2 * max_target_len + 1)\n",
        "\n",
        "    # Compute positions where target[j] != target[j-2]\n",
        "    diff_labels = torch.cat([\n",
        "        torch.as_tensor([[False, False]], device = targets.device).expand(len(batch_arange), -1),\n",
        "        padded_targets[:, 2:] != padded_targets[:, :-2]\n",
        "    ], dim = 1)\n",
        "    # print(diff_labels[0,:7])\n",
        "    # Gather log-probs per input sequence index for tokens in target sequence\n",
        "    log_probs_for_targets = log_probs.gather(-1, padded_targets.unsqueeze(0).repeat(max_input_len, 1, 1))\n",
        "    assert log_probs_for_targets.shape == (max_input_len, batch_size, max_target_len * 2 + 1)\n",
        "\n",
        "    # To avoid nan grad in torch.logsumexp\n",
        "    min_element = torch.finfo(log_probs.dtype).min\n",
        "\n",
        "    # Padding is required at the start of log_alpha for vectorization\n",
        "    # Allows using torch.where for the i=j-2 step of ctc alignment computation\n",
        "    zero_padding = 2\n",
        "\n",
        "    # `log_alpha` is the ctc-probability matrix\n",
        "    # We want to deal in the logarithm-space since the probabilities can end up\n",
        "    # becoming very small and naive multiplication of small numbers can be imprecise.\n",
        "    # Note that usual-multiplication -> log-addition.\n",
        "    # Also note the utility of `logadd` function\n",
        "    log_alpha = torch.full(\n",
        "        size = (max_input_len, batch_size, zero_padding + padded_targets.shape[-1]),\n",
        "        fill_value = min_element, device = log_probs.device, dtype = log_probs.dtype\n",
        "    )\n",
        "    ##############################\n",
        "    # TODO(2) - initialization of ctc-probability matrix\n",
        "    # alpha[0] is non-zero only for first blank (\"_\") and first non-blank token\n",
        "    log_alpha[0, :, zero_padding + 0] = log_probs_for_targets[0, :, 0] ### Complete this\n",
        "    log_alpha[0, :, zero_padding + 1] = log_probs_for_targets[0, :, 1]### Complete this\n",
        "    ##############################\n",
        "\n",
        "\n",
        "    # iterate over input sequence\n",
        "    for t in range(1, max_input_len):\n",
        "        ##############################\n",
        "        # TODO(3) - compute updates using indices j, j-1, j-2; refer to slides\n",
        "        # note that updates for j-2 are conditional upon y[j-2] != y[j], use torch.where\n",
        "        # with a suitable boolean tensor defined above\n",
        "        update_j = log_alpha[t-1,:,zero_padding:] ### Complete this\n",
        "        update_j_minus_1 = log_alpha[t-1,:,zero_padding-1:-1]### Complete this\n",
        "        update_j_minus_2 = log_alpha[t-1,:,zero_padding-2:-2]*diff_labels ### Complete this\n",
        "        update_j_minus_2 = torch.where(update_j_minus_2==0,torch.tensor(min_element),update_j_minus_2)\n",
        "        ##############################\n",
        "\n",
        "\n",
        "        # multiplying with probability is equiv. to adding in the log space\n",
        "        log_alpha[t, :, zero_padding:] = logadd(\n",
        "            update_j,\n",
        "            update_j_minus_1,\n",
        "            update_j_minus_2 #######\n",
        "        ) + log_probs_for_targets[t]\n",
        "\n",
        "\n",
        "    ##############################\n",
        "    # TODO(4) - compute ctc loss terms\n",
        "    # refer to slides, the CTC(x, y) equation - loss is from time step T, tokens 2l, 2l+1\n",
        "    # T might be different for different sequences, use `input_lengths` for that\n",
        "    # l might be different for different sequences, use `target_lengths` for that\n",
        "    x =[]\n",
        "    y = []\n",
        "    for i,a in enumerate(zip(input_lengths,target_lengths)):\n",
        "      x.append(log_alpha[a[0]-1,i,zero_padding+2*a[1]-1])\n",
        "      y.append(log_alpha[a[0]-1,i,zero_padding+2*a[1]])\n",
        "\n",
        "    x = torch.tensor(x)\n",
        "    y = torch.tensor(y)\n",
        "    x = x.reshape((x.shape[0],1))\n",
        "    y = y.reshape((y.shape[0],1))\n",
        "    ctc_loss_term = torch.concatenate((x,y),axis=1)### Complete this computation. You will likely need to define\n",
        "    ### new helper variables to help with this computation.\n",
        "    ##############################\n",
        "    assert ctc_loss_term.shape == (batch_size, 2)\n",
        "\n",
        "    # `path` stores the best hidden state sequence\n",
        "    path = torch.zeros(max_input_len, batch_size, device = log_alpha.device, dtype = torch.int64)\n",
        "\n",
        "    # at timestep T, best index is 2l or 2l+1\n",
        "    path[input_lengths - 1, batch_arange] = zero_padding + 2 * target_lengths - 1 + ctc_loss_term.argmax(dim = -1)\n",
        "\n",
        "    for t, indices in reversed(list(enumerate(path))[1:]):\n",
        "        ##############################\n",
        "        # TODO(5) - compute possible previous states given the current best states in `indices`\n",
        "        # previous states could come from transition i->j where i can be j, j-1 or j-2 (if y[j] != y[j-2])\n",
        "\n",
        "        # if y[j] == y[j-2], use 0 as possible previous state; since 0 is a padding state,\n",
        "        # log_alpha will be very low and argmax below would take care of it.\n",
        "        # such tricks to handle corner-cases help vectorize code in a better fashion.\n",
        "        reshape_index = indices.reshape((indices.shape[0],1))\n",
        "        index_j_2 = diff_labels[list(range(indices.shape[0])),indices-zero_padding].reshape((indices.shape[0],1))\n",
        "        possible_previous_states = torch.concatenate(((reshape_index-2)*index_j_2,reshape_index-1,reshape_index),axis=1)\n",
        "        ##############################\n",
        "        possible_previous_states = torch.nn.functional.relu(possible_previous_states)\n",
        "        assert possible_previous_states.shape == (batch_size, 3)\n",
        "\n",
        "        # get best possible previous state\n",
        "        argmax_among_prev_states = log_alpha[t - 1, batch_arange].gather(-1, possible_previous_states).argmax(dim = -1)\n",
        "        path[t - 1] += (indices - 2 + argmax_among_prev_states).clamp(min = 0)\n",
        "\n",
        "    # color correct (input_index, target_index) cell\n",
        "    # `highlighted_best_path` is a binary (0-1) matrix\n",
        "    highlighted_best_path = torch.zeros_like(log_alpha).scatter_(\n",
        "        dim = -1,\n",
        "        index = path.unsqueeze(-1),\n",
        "        value = 1.0\n",
        "    )[..., (zero_padding + 1):]\n",
        "\n",
        "    # compute unpadded best path\n",
        "    # `highlighted_best_path` can have blank's (\"_\"), we want a mapping to the non-blank tokens only\n",
        "    # (assumed convention) when removing padding, we assign a blank token's positions to the previous token;\n",
        "    # note that the foremost blank token has already been removed above\n",
        "    unpadded_best_path = highlighted_best_path.reshape(max_input_len, batch_size, max_target_len, 2).sum(-1)\n",
        "    assert unpadded_best_path.shape == (max_input_len, batch_size, max_target_len)\n",
        "\n",
        "    return unpadded_best_path"
      ],
      "metadata": {
        "id": "gBADwCu24x3L"
      },
      "execution_count": 310,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Computing CTC alignments using the model results\n",
        "results_in = results.cpu().transpose(0,1)\n",
        "alignment = ctc_alignment(results_in, ground_truth_tokens, input_lens, target_lens, blank = 0)\n",
        "alignment = alignment[:,0, :target_lens[0]]"
      ],
      "metadata": {
        "id": "KBHYK7so-G2j"
      },
      "execution_count": 311,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer is an object of the SentencePieceProcessor class,\n",
        "# Create an array of all the tokens of tokenizer, arranged according to their token IDs\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Get the total number of tokens in the model\n",
        "def array_of_all_tokens(tokenizer):\n",
        "  num_tokens = tokenizer.get_piece_size()\n",
        "\n",
        "  all_tokens_and_ids = [(tokenizer.id_to_piece(token_id), token_id) for token_id in range(num_tokens)]\n",
        "  all_token=[]\n",
        "  for token, token_id in all_tokens_and_ids:\n",
        "      all_token.append(token)\n",
        "  all_tokens=np.array(all_token)\n",
        "  return all_tokens"
      ],
      "metadata": {
        "id": "wGXi20uxgO9N"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################\n",
        "# TODO(6) -  Plot the alignments of the output tokens of the model and the input audio signal\n",
        "# For this, use the ctc_alignments function and the array of all tokens previously created\n",
        "############################\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "def plotting(test_data,all_tokens):\n",
        "  # Returns nothing, displays plot (save as image CTCalignments.png)\n",
        "  # Uses alignment computed two cells back\n",
        "  # Refer to the provided image in the assignment pdf for reference\n",
        "  a = test_data[0][\"tokens\"]\n",
        "  b = all_tokens[a]\n",
        "  ax = sns.heatmap(alignment.T,yticklabels=b)\n",
        "  ax.set_xlabel('Input Audio Timestep')\n",
        "  ax.set_ylabel('Output Tokens')\n",
        "  plt.savefig('CTCalignments.png')"
      ],
      "metadata": {
        "id": "eLwAC5qeYm2M"
      },
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_gt(tokenizer,ground_truth_tokens):\n",
        "  print(ground_truth_tokens[0])\n",
        "  print(tokenizer.decode([int(x) for x in ground_truth_tokens[0]]))"
      ],
      "metadata": {
        "id": "GM0TUMWkDIeD"
      },
      "execution_count": 304,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens=array_of_all_tokens(tokenizer)\n",
        "plotting(test_data,all_tokens)\n",
        "print_gt(tokenizer,ground_truth_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "OaY7zpB9ZGM4",
        "outputId": "07f1b75b-d94a-4561-f7b1-7b5b03eed8d5"
      },
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 20,  35,  16,  10,  10,  19, 135,  92,  26,  85, 109,   2,  43,   3,\n",
            "          3,  74,   9,  17,  98,   9,  70,  38,  12,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n",
            "AND HE ADDED SOMETHING STILL LESS COMPLIMENTARY ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}