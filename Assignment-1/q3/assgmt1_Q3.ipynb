{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3S0Etz_Nokz"
      },
      "source": [
        "\n",
        "# PART I: Running a SpeechBrain ASR Recipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKXIYTJSOnt_"
      },
      "source": [
        "### Setting up the codebase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A5H2lpHX7npZ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Clone SpeechBrain repository\n",
        "!git clone https://github.com/Darshan7575/speechbrain.git\n",
        "%cd /content/speechbrain/\n",
        "\n",
        "# Install required dependencies\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Install SpeechBrain in editable mode\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "61C_gxJo5aRH"
      },
      "outputs": [],
      "source": [
        "# Required imports\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import logging\n",
        "import sys\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "from speechbrain.utils.data_utils import get_all_files, download_file\n",
        "from speechbrain.dataio.dataio import read_audio\n",
        "from speechbrain.utils.distributed import run_on_main, if_main_process\n",
        "\n",
        "# Required variables and loggers\n",
        "logger = logging.getLogger(__name__)\n",
        "logger = logging.getLogger(__name__)\n",
        "MINILIBRI_TRAIN_URL = \"http://www.openslr.org/resources/31/train-clean-5.tar.gz\"\n",
        "MINILIBRI_VALID_URL = \"http://www.openslr.org/resources/31/dev-clean-2.tar.gz\"\n",
        "MINILIBRI_TEST_URL = \"https://www.openslr.org/resources/12/test-clean.tar.gz\"\n",
        "SAMPLERATE = 16000\n",
        "\n",
        "device=\"cuda\"\n",
        "run_opts = {'device':device}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtdr1VnyCQTJ"
      },
      "source": [
        "### Tokenizer Training\n",
        "In this section, we will train a BPE tokenizer with **150 tokens** using `Sentencepiece`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ujToJHWC4T5y"
      },
      "outputs": [],
      "source": [
        "# ############################################################################\n",
        "# Dataset creation helper functions\n",
        "# ############################################################################\n",
        "\n",
        "def prepare_mini_librispeech(\n",
        "    data_folder, save_json_train, save_json_valid, save_json_test\n",
        "):\n",
        "    \"\"\"\n",
        "    Prepares the json files for the Mini Librispeech dataset.\n",
        "    Downloads the dataset if its not found in the `data_folder`.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if this phase is already done (if so, skip it)\n",
        "    if skip(save_json_train, save_json_valid, save_json_test):\n",
        "        logger.info(\"Preparation completed in previous run, skipping.\")\n",
        "        return\n",
        "\n",
        "    # If the dataset doesn't exist yet, download it\n",
        "    train_folder = os.path.join(data_folder, \"LibriSpeech\", \"train-clean-5\")\n",
        "    valid_folder = os.path.join(data_folder, \"LibriSpeech\", \"dev-clean-2\")\n",
        "    test_folder = os.path.join(data_folder, \"LibriSpeech\", \"test-clean\")\n",
        "    if not check_folders(train_folder, valid_folder, test_folder):\n",
        "        download_mini_librispeech(data_folder)\n",
        "\n",
        "    # List files and create manifest from list\n",
        "    logger.info(\n",
        "        f\"Creating {save_json_train}, {save_json_valid}, and {save_json_test}\"\n",
        "    )\n",
        "    extension = [\".flac\"]\n",
        "\n",
        "    # List of flac audio files\n",
        "    wav_list_train = get_all_files(train_folder, match_and=extension)\n",
        "    wav_list_valid = get_all_files(valid_folder, match_and=extension)\n",
        "    wav_list_test = get_all_files(test_folder, match_and=extension)\n",
        "\n",
        "    # List of transcription file\n",
        "    extension = [\".trans.txt\"]\n",
        "    trans_list = get_all_files(data_folder, match_and=extension)\n",
        "    trans_dict = get_transcription(trans_list)\n",
        "\n",
        "    # Create the json files\n",
        "    create_json(wav_list_train, trans_dict, save_json_train)\n",
        "    create_json(wav_list_valid, trans_dict, save_json_valid)\n",
        "    create_json(wav_list_test, trans_dict, save_json_test)\n",
        "\n",
        "\n",
        "def get_transcription(trans_list):\n",
        "    \"\"\"\n",
        "    Returns a dictionary with the transcription of each sentence in the dataset.\n",
        "    \"\"\"\n",
        "    # Processing all the transcription files in the list\n",
        "    trans_dict = {}\n",
        "    for trans_file in trans_list:\n",
        "        # Reading the text file\n",
        "        with open(trans_file) as f:\n",
        "            for line in f:\n",
        "                uttid = line.split(\" \")[0]\n",
        "                text = line.rstrip().split(\" \")[1:]\n",
        "                text = \" \".join(text)\n",
        "                trans_dict[uttid] = text\n",
        "\n",
        "    logger.info(\"Transcription files read!\")\n",
        "    return trans_dict\n",
        "\n",
        "\n",
        "def create_json(wav_list, trans_dict, json_file):\n",
        "    \"\"\"\n",
        "    Creates the json file given a list of wav files and their transcriptions.\n",
        "    \"\"\"\n",
        "    # Processing all the wav files in the list\n",
        "    json_dict = {}\n",
        "    for wav_file in wav_list:\n",
        "\n",
        "        # Reading the signal (to retrieve duration in seconds)\n",
        "        signal = read_audio(wav_file)\n",
        "        duration = signal.shape[0] / SAMPLERATE\n",
        "\n",
        "        # Manipulate path to get relative path and uttid\n",
        "        path_parts = wav_file.split(os.path.sep)\n",
        "        uttid, _ = os.path.splitext(path_parts[-1])\n",
        "        relative_path = os.path.join(\"{data_root}\", *path_parts[-5:])\n",
        "\n",
        "        # Create entry for this utterance\n",
        "        json_dict[uttid] = {\n",
        "            \"wav\": relative_path,\n",
        "            \"length\": duration,\n",
        "            \"words\": trans_dict[uttid],\n",
        "        }\n",
        "\n",
        "    # Writing the dictionary to the json file\n",
        "    with open(json_file, mode=\"w\") as json_f:\n",
        "        json.dump(json_dict, json_f, indent=2)\n",
        "\n",
        "    logger.info(f\"{json_file} successfully created!\")\n",
        "\n",
        "\n",
        "def skip(*filenames):\n",
        "    \"\"\"\n",
        "    Detects if the data preparation has been already done.\n",
        "    If the preparation has been done, we can skip it.\n",
        "    \"\"\"\n",
        "    for filename in filenames:\n",
        "        if not os.path.isfile(filename):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def check_folders(*folders):\n",
        "    \"\"\"Returns False if any passed folder does not exist.\"\"\"\n",
        "    for folder in folders:\n",
        "        if not os.path.exists(folder):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def download_mini_librispeech(destination):\n",
        "    \"\"\"Download dataset and unpack it.\n",
        "    \"\"\"\n",
        "    train_archive = os.path.join(destination, \"train-clean-5.tar.gz\")\n",
        "    valid_archive = os.path.join(destination, \"dev-clean-2.tar.gz\")\n",
        "    test_archive = os.path.join(destination, \"test-clean.tar.gz\")\n",
        "    download_file(MINILIBRI_TRAIN_URL, train_archive)\n",
        "    download_file(MINILIBRI_VALID_URL, valid_archive)\n",
        "    download_file(MINILIBRI_TEST_URL, test_archive)\n",
        "    shutil.unpack_archive(train_archive, destination)\n",
        "    shutil.unpack_archive(valid_archive, destination)\n",
        "    shutil.unpack_archive(test_archive, destination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rz9pyHan4V0S"
      },
      "outputs": [],
      "source": [
        "tokenizer_hyperparams = \"\"\"\n",
        "# ############################################################################\n",
        "# Tokenizer: subword BPE with unigram 150\n",
        "# ############################################################################\n",
        "\n",
        "output_folder: !ref results/tokenizer/\n",
        "\n",
        "# Data files\n",
        "data_folder: data\n",
        "train_annotation: !ref <data_folder>/train.json\n",
        "valid_annotation: !ref <data_folder>/valid.json\n",
        "test_annotation: !ref <data_folder>/test.json\n",
        "\n",
        "# Tokenizer training parameters\n",
        "token_type: unigram  # [\"unigram\", \"bpe\", \"char\"]\n",
        "token_output: 150  # index(blank/eos/bos/unk) = 0\n",
        "character_coverage: 1.0\n",
        "json_read: words\n",
        "\n",
        "tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece\n",
        "   model_dir: !ref <output_folder>\n",
        "   vocab_size: !ref <token_output>\n",
        "   annotation_train: !ref <train_annotation>\n",
        "   annotation_read: !ref <json_read>\n",
        "   annotation_format: json\n",
        "   model_type: !ref <token_type> # [\"unigram\", \"bpe\", \"char\"]\n",
        "   character_coverage: !ref <character_coverage>\n",
        "   annotation_list_to_check: [!ref <train_annotation>, !ref <valid_annotation>]\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "nA8xD6Do4Xl7",
        "outputId": "09ffa455-1a9e-4707-9e7d-a3623350c14a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/tokenizer/\n",
            "Downloading http://www.openslr.org/resources/31/train-clean-5.tar.gz to data/train-clean-5.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train-clean-5.tar.gz: 333MB [00:14, 23.4MB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://www.openslr.org/resources/31/dev-clean-2.tar.gz to data/dev-clean-2.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "dev-clean-2.tar.gz: 126MB [00:05, 21.9MB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.openslr.org/resources/12/test-clean.tar.gz to data/test-clean.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test-clean.tar.gz: 347MB [00:14, 24.3MB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__main__ - Creating data/train.json, data/valid.json, and data/test.json\n",
            "__main__ - Transcription files read!\n",
            "__main__ - data/train.json successfully created!\n",
            "__main__ - data/valid.json successfully created!\n",
            "__main__ - data/test.json successfully created!\n",
            "speechbrain.tokenizers.SentencePiece - Train tokenizer with type:unigram\n",
            "speechbrain.tokenizers.SentencePiece - Extract words sequences from:data/train.json\n",
            "speechbrain.tokenizers.SentencePiece - Text file created at: results/tokenizer/train.txt\n",
            "speechbrain.tokenizers.SentencePiece - ==== Loading Tokenizer ===\n",
            "speechbrain.tokenizers.SentencePiece - Tokenizer path: results/tokenizer/150_unigram.model\n",
            "speechbrain.tokenizers.SentencePiece - Tokenizer vocab_size: 150\n",
            "speechbrain.tokenizers.SentencePiece - Tokenizer type: unigram\n",
            "speechbrain.tokenizers.SentencePiece - ==== Accuracy checking for recovering text from tokenizer ===\n",
            "speechbrain.tokenizers.SentencePiece - recover words from: data/train.json\n",
            "speechbrain.tokenizers.SentencePiece - Wrong recover words: 0\n",
            "speechbrain.tokenizers.SentencePiece - accuracy recovering words: 1.0\n",
            "speechbrain.tokenizers.SentencePiece - ==== Accuracy checking for recovering text from tokenizer ===\n",
            "speechbrain.tokenizers.SentencePiece - recover words from: data/valid.json\n",
            "speechbrain.tokenizers.SentencePiece - Wrong recover words: 0\n",
            "speechbrain.tokenizers.SentencePiece - accuracy recovering words: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'results/tokenizer//tokenizer.ckpt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# load required params from the hyperpyyaml file\n",
        "hparams = load_hyperpyyaml(tokenizer_hyperparams)\n",
        "\n",
        "# 1. Dataset creation\n",
        "\n",
        "## Create experiment directory\n",
        "sb.create_experiment_directory(\n",
        "    experiment_directory=hparams[\"output_folder\"],\n",
        "    overrides=None,\n",
        ")\n",
        "\n",
        "## Create dataset\n",
        "run_on_main(\n",
        "    prepare_mini_librispeech,\n",
        "    kwargs={\n",
        "        \"data_folder\": hparams[\"data_folder\"],\n",
        "        \"save_json_train\": hparams[\"train_annotation\"],\n",
        "        \"save_json_valid\": hparams[\"valid_annotation\"],\n",
        "        \"save_json_test\": hparams[\"test_annotation\"],\n",
        "    },\n",
        ")\n",
        "\n",
        "# 2. Tokenizer training\n",
        "hparams[\"tokenizer\"]()\n",
        "\n",
        "# 3. Saving tokenizer in .ckpt extension\n",
        "output_path = hparams[\"output_folder\"]\n",
        "token_output = hparams[\"token_output\"]\n",
        "token_type = hparams[\"token_type\"]\n",
        "bpe_model = f\"{output_path}/{token_output}_{token_type}.model\"\n",
        "tokenizer_ckpt = f\"{output_path}/tokenizer.ckpt\"\n",
        "shutil.copyfile(bpe_model, tokenizer_ckpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwqFx3QcOdd3"
      },
      "source": [
        "### Model Training\n",
        "In this section, we will train a **6 layer Conformer** encoder only architecture with the `CTC objective`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6WPcwXueCRm7"
      },
      "outputs": [],
      "source": [
        "global_hyperparams = \"\"\"\n",
        "# Seed needs to be set at top of yaml, before objects with parameters are made\n",
        "seed: 2024\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Data files\n",
        "data_folder: data\n",
        "train_annotation: !ref <data_folder>/train.json\n",
        "valid_annotation: !ref <data_folder>/valid.json\n",
        "test_annotation: !ref <data_folder>/test.json\n",
        "\n",
        "# Language model (LM) pretraining\n",
        "pretrained_lm_tokenizer_path: ./results/tokenizer\n",
        "\n",
        "# Training parameters\n",
        "number_of_epochs: 30\n",
        "batch_size: 8\n",
        "lr_adam: 0.001\n",
        "max_grad_norm: 5.0\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "loss_reduction: 'batchmean'\n",
        "\n",
        "# Dataloader options\n",
        "train_dataloader_opts:\n",
        "    batch_size: !ref <batch_size>\n",
        "\n",
        "valid_dataloader_opts:\n",
        "    batch_size: !ref <batch_size>\n",
        "\n",
        "test_dataloader_opts:\n",
        "    batch_size: !ref <batch_size>\n",
        "\n",
        "# Feature parameters\n",
        "sample_rate: 16000\n",
        "n_fft: 400\n",
        "n_mels: 80\n",
        "\n",
        "####################### Model parameters ###########################\n",
        "# Transformer\n",
        "d_model: 128\n",
        "nhead: 4\n",
        "num_encoder_layers: 6\n",
        "d_ffn: 512\n",
        "transformer_dropout: 0.1\n",
        "activation: !name:torch.nn.GELU\n",
        "output_neurons: 150\n",
        "label_smoothing: 0.0\n",
        "attention_type: RelPosMHAXL\n",
        "\n",
        "# Outputs\n",
        "blank_index: 0\n",
        "pad_index: 0\n",
        "bos_index: 1\n",
        "eos_index: 2\n",
        "\n",
        "# Decoding parameters\n",
        "min_decode_ratio: 0.0\n",
        "max_decode_ratio: 1.0\n",
        "test_beam_size: 1\n",
        "ctc_weight_decode: 1.0\n",
        "\n",
        "############################## models ################################\n",
        "\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    sample_rate: !ref <sample_rate>\n",
        "    n_fft: !ref <n_fft>\n",
        "    n_mels: !ref <n_mels>\n",
        "\n",
        "CNN: !new:speechbrain.lobes.models.convolution.ConvolutionFrontEnd\n",
        "    input_shape: (8, 10, 80)\n",
        "    num_blocks: 2\n",
        "    num_layers_per_block: 1\n",
        "    out_channels: (64, 32)\n",
        "    kernel_sizes: (3, 3)\n",
        "    strides: (2, 2)\n",
        "    residuals: (False, False)\n",
        "\n",
        "# standard parameters for the BASE model\n",
        "Transformer: !new:speechbrain.lobes.models.transformer.TransformerASR.TransformerASR\n",
        "    input_size: 640\n",
        "    tgt_vocab: !ref <output_neurons>\n",
        "    d_model: !ref <d_model>\n",
        "    nhead: !ref <nhead>\n",
        "    num_encoder_layers: !ref <num_encoder_layers>\n",
        "    num_decoder_layers: 0\n",
        "    d_ffn: !ref <d_ffn>\n",
        "    dropout: !ref <transformer_dropout>\n",
        "    activation: !ref <activation>\n",
        "    encoder_module: conformer\n",
        "    attention_type: !ref <attention_type>\n",
        "    normalize_before: True\n",
        "\n",
        "tokenizer: !new:sentencepiece.SentencePieceProcessor\n",
        "\n",
        "ctc_lin: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <d_model>\n",
        "    n_neurons: !ref <output_neurons>\n",
        "\n",
        "normalize: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: global\n",
        "    update_until_epoch: 4\n",
        "\n",
        "modules:\n",
        "    CNN: !ref <CNN>\n",
        "    Transformer: !ref <Transformer>\n",
        "    ctc_lin: !ref <ctc_lin>\n",
        "    normalize: !ref <normalize>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <CNN>, !ref <Transformer>, !ref <ctc_lin>]\n",
        "\n",
        "# define two optimizers here for two-stage training\n",
        "Adam: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_adam>\n",
        "    betas: (0.9, 0.98)\n",
        "    eps: 0.000000001\n",
        "\n",
        "log_softmax: !new:torch.nn.LogSoftmax\n",
        "    dim: -1\n",
        "\n",
        "ctc_cost: !name:speechbrain.nnet.losses.ctc_loss\n",
        "    blank_index: !ref <blank_index>\n",
        "    reduction: !ref <loss_reduction>\n",
        "\n",
        "noam_annealing: !new:speechbrain.nnet.schedulers.NoamScheduler\n",
        "    lr_initial: !ref <lr_adam>\n",
        "    n_warmup_steps: 1500\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "error_rate_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats\n",
        "\n",
        "cer_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats\n",
        "   split_tokens: True\n",
        "\n",
        "# The pretrainer allows a mapping between pretrained files and instances that\n",
        "# are declared in the yaml. E.g here, we will download the file tokenizer.ckpt\n",
        "# and it will be loaded into \"tokenizer\" which is pointing to the <pretrained_lm_tokenizer_path> defined\n",
        "# before.\n",
        "pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer\n",
        "    loadables:\n",
        "        tokenizer: !ref <tokenizer>\n",
        "    paths:\n",
        "        tokenizer: !ref <pretrained_lm_tokenizer_path>/tokenizer.ckpt\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "euJMqDLSWv7Y"
      },
      "outputs": [],
      "source": [
        "def dataio_prepare(hparams):\n",
        "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
        "    It also defines the data processing pipeline through user-defined functions.\n",
        "    \"\"\"\n",
        "    # Define audio pipeline. In this case, we simply read the path contained\n",
        "    # in the variable wav with the audio reader.\n",
        "    @sb.utils.data_pipeline.takes(\"wav\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav):\n",
        "        \"\"\"Load the audio signal. This is done on the CPU in the `collate_fn`.\"\"\"\n",
        "        sig = sb.dataio.dataio.read_audio(wav)\n",
        "        return sig\n",
        "\n",
        "    tokenizer = hparams[\"tokenizer\"]\n",
        "    # Define text processing pipeline. We start from the raw text and then\n",
        "    # encode it using the tokenizer. The tokens with BOS are used for feeding\n",
        "    # decoder during training, the tokens with EOS for computing the cost function.\n",
        "    # The tokens without BOS or EOS is for computing CTC loss.\n",
        "    @sb.utils.data_pipeline.takes(\"words\")\n",
        "    @sb.utils.data_pipeline.provides(\n",
        "        \"wrd\", \"tokens_list\", \"tokens_bos\", \"tokens_eos\", \"tokens\"\n",
        "    )\n",
        "    def text_pipeline(wrd):\n",
        "        \"\"\"Processes the transcriptions to generate proper labels\"\"\"\n",
        "        yield wrd\n",
        "        tokens_list = tokenizer.encode_as_ids(wrd)\n",
        "        yield tokens_list\n",
        "        tokens_bos = torch.LongTensor([hparams[\"bos_index\"]] + (tokens_list))\n",
        "        yield tokens_bos\n",
        "        tokens_eos = torch.LongTensor(tokens_list + [hparams[\"eos_index\"]])\n",
        "        yield tokens_eos\n",
        "        tokens = torch.LongTensor(tokens_list)\n",
        "        yield tokens\n",
        "\n",
        "    # Define datasets from json data manifest file\n",
        "    # Define datasets sorted by ascending lengths for efficiency\n",
        "    datasets = {}\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "    for dataset in [\"train\", \"valid\", \"test\"]:\n",
        "        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
        "            json_path=hparams[f\"{dataset}_annotation\"],\n",
        "            replacements={\"data_root\": data_folder},\n",
        "            dynamic_items=[audio_pipeline, text_pipeline],\n",
        "            output_keys=[\n",
        "                \"id\",\n",
        "                \"sig\",\n",
        "                \"wrd\",\n",
        "                \"tokens_bos\",\n",
        "                \"tokens_eos\",\n",
        "                \"tokens\",\n",
        "            ],\n",
        "        )\n",
        "        hparams[f\"{dataset}_dataloader_opts\"][\"shuffle\"] = False\n",
        "\n",
        "    datasets[\"train\"] = datasets[\"train\"].filtered_sorted(sort_key=\"length\")\n",
        "    hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
        "\n",
        "    return (\n",
        "        datasets[\"train\"],\n",
        "        datasets[\"valid\"],\n",
        "        datasets[\"test\"],\n",
        "        tokenizer\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wUEBBUOQZ4V-"
      },
      "outputs": [],
      "source": [
        "# Define training procedure\n",
        "class BaseASR(sb.Brain):\n",
        "    def __init__(\n",
        "        self,\n",
        "        modules=None,\n",
        "        opt_class=None,\n",
        "        hparams=None,\n",
        "        run_opts=None,\n",
        "        checkpointer=None,\n",
        "        profiler=None,\n",
        "        tokenizer=None,\n",
        "    ):\n",
        "        super(BaseASR, self).__init__(\n",
        "            modules=modules,\n",
        "            opt_class=opt_class,\n",
        "            hparams=hparams,\n",
        "            run_opts=run_opts,\n",
        "            checkpointer=checkpointer,\n",
        "            profiler=profiler\n",
        "        )\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Performs a forward pass through the encoder\"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, wav_lens = batch.sig\n",
        "        tokens_bos, _ = batch.tokens_bos\n",
        "\n",
        "        # compute features\n",
        "        feats = self.hparams.compute_features(wavs)#### FILL THIS WITH YOUR CODE FROM PART I ####\n",
        "        current_epoch = self.hparams.epoch_counter.current\n",
        "        feats = self.modules.normalize(feats, wav_lens, epoch=current_epoch)\n",
        "\n",
        "        # forward modules\n",
        "        src = self.modules.CNN(feats)\n",
        "\n",
        "        enc_out, _ = self.modules.Transformer(\n",
        "            src, tokens_bos, wav_lens, pad_idx=self.hparams.pad_index,\n",
        "        )\n",
        "\n",
        "        # output layer for ctc log-probabilities\n",
        "        logits = self.modules.ctc_lin(enc_out)\n",
        "        loss_func = sb.nnet.activations.Softmax(apply_log=True)\n",
        "        p_ctc = loss_func.forward(logits)######\n",
        "\n",
        "        # Compute outputs\n",
        "        hyps = None\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            hyps = None\n",
        "        else:\n",
        "            hyps = sb.decoders.ctc_greedy_decode(\n",
        "                p_ctc, wav_lens, blank_id=self.hparams.blank_index\n",
        "            )\n",
        "\n",
        "        return p_ctc, wav_lens, hyps\n",
        "\n",
        "    def get_ctc_probs(self, batch):\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, wav_lens = batch.sig\n",
        "        tokens_bos, _ = batch.tokens_bos\n",
        "\n",
        "        # compute features\n",
        "        feats = self.hparams.compute_features(wavs)\n",
        "        current_epoch = self.hparams.epoch_counter.current\n",
        "        feats = self.modules.normalize(feats, wav_lens, epoch=current_epoch)\n",
        "\n",
        "        # forward modules\n",
        "        src = self.modules.CNN(feats)\n",
        "\n",
        "        enc_out, _ = self.modules.Transformer(\n",
        "            src, tokens_bos, wav_lens, pad_idx=self.hparams.pad_index,\n",
        "        )\n",
        "\n",
        "        logits = self.modules.ctc_lin(enc_out)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the CTC loss given predictions and targets.\"\"\"\n",
        "\n",
        "        (p_ctc, wav_lens, hyps,) = predictions\n",
        "\n",
        "        ids = batch.id\n",
        "        tokens_eos, tokens_eos_lens = batch.tokens_eos\n",
        "        tokens, tokens_lens = batch.tokens\n",
        "\n",
        "        # Calculate CTC loss\n",
        "         #### FILL THIS WITH YOUR CODE FROM PART I ####\n",
        "        loss = self.hparams.ctc_cost(p_ctc, tokens, wav_lens, tokens_lens)  ######\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            # Decode token terms to words\n",
        "            predicted_words = [\n",
        "                self.tokenizer.decode_ids(utt_seq).split(\" \") for utt_seq in hyps\n",
        "            ]\n",
        "            target_words = [wrd.split(\" \") for wrd in batch.wrd]\n",
        "            self.wer_metric.append(ids, predicted_words, target_words)\n",
        "            self.cer_metric.append(ids, predicted_words, target_words)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_evaluate_start(self, max_key=None, min_key=None):\n",
        "        \"\"\"Performs checkpoint averge if needed\"\"\"\n",
        "        super().on_evaluate_start()\n",
        "\n",
        "        ckpts = self.checkpointer.find_checkpoints(\n",
        "            max_key=max_key, min_key=min_key\n",
        "        )\n",
        "        ckpt = sb.utils.checkpoints.average_checkpoints(\n",
        "            ckpts, recoverable_name=\"model\", device=self.device\n",
        "        )\n",
        "\n",
        "        self.hparams.model.load_state_dict(ckpt, strict=True)\n",
        "        self.hparams.model.eval()\n",
        "        print(\"Loaded the average\")\n",
        "\n",
        "    def evaluate_batch(self, batch, stage):\n",
        "        \"\"\"Computations needed for validation/test batches\"\"\"\n",
        "        with torch.no_grad():\n",
        "            predictions = self.compute_forward(batch, stage=stage)\n",
        "            loss = self.compute_objectives(predictions, batch, stage=stage)\n",
        "        return loss.detach()\n",
        "\n",
        "    def on_stage_start(self, stage, epoch):\n",
        "        \"\"\"Gets called at the beginning of each epoch\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.cer_metric = self.hparams.cer_computer()\n",
        "            self.wer_metric = self.hparams.error_rate_computer()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch):\n",
        "        \"\"\"Gets called at the end of a epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"CER\"] = self.cer_metric.summarize(\"error_rate\")\n",
        "            stage_stats[\"WER\"] = self.wer_metric.summarize(\"error_rate\")\n",
        "\n",
        "        # log stats and save checkpoint at end-of-epoch\n",
        "        if stage == sb.Stage.VALID and sb.utils.distributed.if_main_process():\n",
        "\n",
        "            lr = self.hparams.noam_annealing.current_lr\n",
        "            steps = self.optimizer_step\n",
        "            optimizer = self.optimizer.__class__.__name__\n",
        "\n",
        "            epoch_stats = {\n",
        "                \"epoch\": epoch,\n",
        "                \"lr\": lr,\n",
        "                \"steps\": steps,\n",
        "                \"optimizer\": optimizer,\n",
        "            }\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta=epoch_stats,\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            # Save only last 10 checkpoints\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"loss\": stage_loss, \"epoch\": epoch},\n",
        "                max_keys=[\"epoch\"],\n",
        "                num_to_keep=10,\n",
        "            )\n",
        "\n",
        "        elif stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "            # Write the WER metric for test dataset\n",
        "            if if_main_process():\n",
        "                with open(self.hparams.test_wer_file, \"w\") as w:\n",
        "                    self.wer_metric.write_stats(w)\n",
        "\n",
        "    def fit_batch(self, batch):\n",
        "        \"\"\"Performs a forward + backward pass on 1 batch\n",
        "        \"\"\"\n",
        "\n",
        "        should_step = self.step % self.grad_accumulation_factor == 0\n",
        "\n",
        "        outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
        "        loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
        "        loss.backward()\n",
        "        if self.check_gradients(loss):\n",
        "            self.optimizer.step()\n",
        "        self.zero_grad()\n",
        "        self.optimizer_step += 1\n",
        "        self.hparams.noam_annealing(self.optimizer)\n",
        "\n",
        "        self.on_fit_batch_end(batch, outputs, loss, should_step)\n",
        "        return loss.detach().cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DDQEQ8M2MNAi"
      },
      "outputs": [],
      "source": [
        "task_hyperparameters = \"\"\"\n",
        "# Setup the directory to host experiment results\n",
        "output_folder: !ref results/transformer/Task_1\n",
        "wer_file: !ref <output_folder>/wer.txt\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        noam_scheduler: !ref <noam_annealing>\n",
        "        normalizer: !ref <normalize>\n",
        "        counter: !ref <epoch_counter>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tGonNC7u8hlJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "707dd1f9-ffef-457c-d4ed-adb092ea5cb8"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/transformer/Task_1\n",
            "speechbrain.pretrained.fetching - Destination tokenizer.ckpt: local file in /content/speechbrain/results/tokenizer/tokenizer.ckpt.\n",
            "speechbrain.utils.parameter_transfer - Set local path in self.paths[tokenizer] = model_checkpoints/tokenizer.ckpt\n",
            "speechbrain.utils.parameter_transfer - Loading pretrained files for: tokenizer\n",
            "speechbrain.utils.parameter_transfer - Redirecting (loading from local path): model_checkpoints/tokenizer.ckpt -> model_checkpoints/tokenizer.ckpt\n",
            "speechbrain.core - Info: max_grad_norm arg from hparam file is used\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - 2.6M trainable parameters in BaseASR\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:38<00:00,  4.92it/s, train_loss=474]\n",
            "100%|██████████| 137/137 [00:09<00:00, 14.26it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 1, lr: 1.26e-04, steps: 190, optimizer: Adam - train loss: 4.74e+02 - valid loss: 2.35e+02, valid CER: 1.00e+02, valid WER: 1.00e+02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-22-42+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 190/190 [00:27<00:00,  6.86it/s, train_loss=431]\n",
            "100%|██████████| 137/137 [00:09<00:00, 15.10it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 2, lr: 2.53e-04, steps: 380, optimizer: Adam - train loss: 4.31e+02 - valid loss: 2.34e+02, valid CER: 1.00e+02, valid WER: 1.00e+02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-23-19+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 190/190 [00:29<00:00,  6.46it/s, train_loss=430]\n",
            "100%|██████████| 137/137 [00:08<00:00, 15.67it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 3, lr: 3.79e-04, steps: 570, optimizer: Adam - train loss: 4.30e+02 - valid loss: 2.32e+02, valid CER: 1.00e+02, valid WER: 1.00e+02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-23-57+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 190/190 [00:28<00:00,  6.74it/s, train_loss=409]\n",
            "100%|██████████| 137/137 [00:10<00:00, 12.98it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 4, lr: 5.06e-04, steps: 760, optimizer: Adam - train loss: 4.09e+02 - valid loss: 2.08e+02, valid CER: 85.50, valid WER: 96.83\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-24-36+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:27<00:00,  6.90it/s, train_loss=351]\n",
            "100%|██████████| 137/137 [00:12<00:00, 11.09it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 5, lr: 6.33e-04, steps: 950, optimizer: Adam - train loss: 3.51e+02 - valid loss: 1.78e+02, valid CER: 71.13, valid WER: 92.92\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-25-16+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 190/190 [00:27<00:00,  6.88it/s, train_loss=306]\n",
            "100%|██████████| 137/137 [00:13<00:00, 10.34it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 6, lr: 7.59e-04, steps: 1140, optimizer: Adam - train loss: 3.06e+02 - valid loss: 1.59e+02, valid CER: 62.55, valid WER: 91.17\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-25-57+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.epoch_loop - Going into epoch 7\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.70it/s, train_loss=273]\n",
            "100%|██████████| 137/137 [00:14<00:00,  9.71it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 7, lr: 8.86e-04, steps: 1330, optimizer: Adam - train loss: 2.73e+02 - valid loss: 1.45e+02, valid CER: 56.15, valid WER: 88.31\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-26-40+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.epoch_loop - Going into epoch 8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:27<00:00,  6.84it/s, train_loss=245]\n",
            "100%|██████████| 137/137 [00:14<00:00,  9.24it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 8, lr: 9.94e-04, steps: 1520, optimizer: Adam - train loss: 2.45e+02 - valid loss: 1.34e+02, valid CER: 50.92, valid WER: 85.91\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-27-23+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 9\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:27<00:00,  6.80it/s, train_loss=220]\n",
            "100%|██████████| 137/137 [00:14<00:00,  9.30it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 9, lr: 9.37e-04, steps: 1710, optimizer: Adam - train loss: 2.20e+02 - valid loss: 1.25e+02, valid CER: 47.25, valid WER: 84.24\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-28-06+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.epoch_loop - Going into epoch 10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:27<00:00,  6.82it/s, train_loss=199]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.49it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 10, lr: 8.89e-04, steps: 1900, optimizer: Adam - train loss: 1.99e+02 - valid loss: 1.19e+02, valid CER: 44.09, valid WER: 81.78\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-28-50+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.epoch_loop - Going into epoch 11\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:27<00:00,  6.84it/s, train_loss=181]\n",
            "100%|██████████| 137/137 [00:15<00:00,  9.04it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 11, lr: 8.47e-04, steps: 2090, optimizer: Adam - train loss: 1.81e+02 - valid loss: 1.15e+02, valid CER: 42.85, valid WER: 80.41\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-29-33+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-22-42+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 12\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.69it/s, train_loss=166]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.99it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 12, lr: 8.11e-04, steps: 2280, optimizer: Adam - train loss: 1.66e+02 - valid loss: 1.11e+02, valid CER: 41.40, valid WER: 79.63\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-30-17+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-23-19+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 13\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:27<00:00,  6.82it/s, train_loss=153]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.32it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 13, lr: 7.79e-04, steps: 2470, optimizer: Adam - train loss: 1.53e+02 - valid loss: 1.10e+02, valid CER: 40.03, valid WER: 78.64\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-31-02+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-23-57+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:27<00:00,  6.84it/s, train_loss=141]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.61it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 14, lr: 7.51e-04, steps: 2660, optimizer: Adam - train loss: 1.41e+02 - valid loss: 1.10e+02, valid CER: 39.39, valid WER: 77.71\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-31-46+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-24-36+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.67it/s, train_loss=131]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.89it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 15, lr: 7.26e-04, steps: 2850, optimizer: Adam - train loss: 1.31e+02 - valid loss: 1.07e+02, valid CER: 38.32, valid WER: 76.21\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-32-30+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-25-16+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 16\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:27<00:00,  6.80it/s, train_loss=121]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.21it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 16, lr: 7.03e-04, steps: 3040, optimizer: Adam - train loss: 1.21e+02 - valid loss: 1.08e+02, valid CER: 37.40, valid WER: 75.88\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-33-15+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-25-57+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 17\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:27<00:00,  6.80it/s, train_loss=113]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.77it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 17, lr: 6.82e-04, steps: 3230, optimizer: Adam - train loss: 1.13e+02 - valid loss: 1.07e+02, valid CER: 37.27, valid WER: 75.51\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-33-59+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-26-40+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 18\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.68it/s, train_loss=105]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.61it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 18, lr: 6.62e-04, steps: 3420, optimizer: Adam - train loss: 1.05e+02 - valid loss: 1.08e+02, valid CER: 36.70, valid WER: 75.16\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-34-44+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-27-23+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 19\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:27<00:00,  6.82it/s, train_loss=98.7]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.43it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 19, lr: 6.45e-04, steps: 3610, optimizer: Adam - train loss: 98.71 - valid loss: 1.08e+02, valid CER: 36.40, valid WER: 74.77\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-35-28+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-28-06+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:27<00:00,  6.93it/s, train_loss=92.2]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 20, lr: 6.28e-04, steps: 3800, optimizer: Adam - train loss: 92.17 - valid loss: 1.10e+02, valid CER: 36.45, valid WER: 74.40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-36-12+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-28-50+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.78it/s, train_loss=86.4]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 21, lr: 6.13e-04, steps: 3990, optimizer: Adam - train loss: 86.38 - valid loss: 1.12e+02, valid CER: 36.52, valid WER: 74.38\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-36-57+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-29-33+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.68it/s, train_loss=81.3]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 22, lr: 5.99e-04, steps: 4180, optimizer: Adam - train loss: 81.26 - valid loss: 1.13e+02, valid CER: 36.06, valid WER: 74.00\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-37-42+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-30-17+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.76it/s, train_loss=76.3]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 23, lr: 5.86e-04, steps: 4370, optimizer: Adam - train loss: 76.32 - valid loss: 1.14e+02, valid CER: 35.76, valid WER: 73.40\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-38-27+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-31-02+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:27<00:00,  6.79it/s, train_loss=71.5]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 24, lr: 5.74e-04, steps: 4560, optimizer: Adam - train loss: 71.46 - valid loss: 1.15e+02, valid CER: 35.45, valid WER: 73.31\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-39-12+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-31-46+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.77it/s, train_loss=67.5]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 25, lr: 5.62e-04, steps: 4750, optimizer: Adam - train loss: 67.45 - valid loss: 1.16e+02, valid CER: 35.22, valid WER: 73.34\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-39-56+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-32-30+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.70it/s, train_loss=63.7]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 26, lr: 5.51e-04, steps: 4940, optimizer: Adam - train loss: 63.73 - valid loss: 1.17e+02, valid CER: 34.76, valid WER: 72.28\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-40-41+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-33-15+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.67it/s, train_loss=60.1]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 27, lr: 5.41e-04, steps: 5130, optimizer: Adam - train loss: 60.07 - valid loss: 1.19e+02, valid CER: 34.52, valid WER: 72.06\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-41-26+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-33-59+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.76it/s, train_loss=56.7]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 28, lr: 5.31e-04, steps: 5320, optimizer: Adam - train loss: 56.65 - valid loss: 1.21e+02, valid CER: 34.45, valid WER: 72.20\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-42-12+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-34-44+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:28<00:00,  6.76it/s, train_loss=53.6]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 29, lr: 5.22e-04, steps: 5510, optimizer: Adam - train loss: 53.60 - valid loss: 1.21e+02, valid CER: 34.28, valid WER: 72.39\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-42-57+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-35-28+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:27<00:00,  6.79it/s, train_loss=51.2]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 30, lr: 5.13e-04, steps: 5700, optimizer: Adam - train loss: 51.25 - valid loss: 1.23e+02, valid CER: 34.09, valid WER: 72.80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-43-42+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_1/save/CKPT+2024-02-18+12-36-12+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/transformer/Task_1/save/CKPT+2024-02-18+12-43-42+00\n",
            "Loaded the average\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:44<00:00,  4.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - Epoch loaded: 30 - test loss: 28.86, test CER: 6.13, test WER: 21.05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28.858904810720382"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "hyperparams = global_hyperparams + task_hyperparameters\n",
        "hparams = load_hyperpyyaml(hyperparams)\n",
        "\n",
        "# Create experiment directory\n",
        "sb.create_experiment_directory(\n",
        "    experiment_directory=hparams[\"output_folder\"],\n",
        "    overrides=None,\n",
        ")\n",
        "\n",
        "# Here we create the datasets objects as well as tokenization and encoding\n",
        "(\n",
        "    train_data,\n",
        "    valid_data,\n",
        "    test_data,\n",
        "    tokenizer\n",
        ") = dataio_prepare(hparams)\n",
        "\n",
        "# We download the pretrained LM from HuggingFace (or elsewhere depending on\n",
        "# the path given in the YAML file). The tokenizer is loaded at the same time.\n",
        "run_on_main(hparams[\"pretrainer\"].collect_files)\n",
        "hparams[\"pretrainer\"].load_collected(device=run_opts[\"device\"])\n",
        "\n",
        "# Trainer initialization\n",
        "asr_brain = BaseASR(\n",
        "    modules=hparams[\"modules\"],\n",
        "    opt_class=hparams[\"Adam\"],\n",
        "    hparams=hparams,\n",
        "    checkpointer=hparams[\"checkpointer\"],\n",
        "    run_opts=run_opts,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# adding objects to trainer:\n",
        "train_dataloader_opts = hparams[\"train_dataloader_opts\"]\n",
        "valid_dataloader_opts = hparams[\"valid_dataloader_opts\"]\n",
        "\n",
        "# Training\n",
        "asr_brain.fit(\n",
        "    asr_brain.hparams.epoch_counter,\n",
        "    train_data,\n",
        "    valid_data,\n",
        "    train_loader_kwargs=train_dataloader_opts,\n",
        "    valid_loader_kwargs=valid_dataloader_opts\n",
        ")\n",
        "\n",
        "# Testing\n",
        "asr_brain.hparams.test_wer_file = asr_brain.hparams.wer_file\n",
        "asr_brain.evaluate(\n",
        "    train_data,\n",
        "    max_key=\"epoch\",\n",
        "    test_loader_kwargs=hparams[\"train_dataloader_opts\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbkMK32DJpHZ"
      },
      "source": [
        "# PART III: Visualizing CTC alignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zIfWCEVTJvYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed0dd958-2455-4505-c8ab-25ac44dce3eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 538, 150])\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "(\n",
        "    train_data,\n",
        "    valid_data,\n",
        "    test_data,\n",
        "    _\n",
        ") = dataio_prepare(hparams)\n",
        "\n",
        "#Creating a test dataloader to get model results on test audio\n",
        "hparams['train_dataloader_opts']['batch_size'] = 8\n",
        "test_loader = asr_brain.make_dataloader(test_data, sb.Stage.TEST, **hparams[\"train_dataloader_opts\"])\n",
        "results = None\n",
        "\n",
        "#Creating a batch for the test dataloader and getting model results\n",
        "for batch in test_loader:\n",
        "    ground_truth_tokens = batch.tokens[0]\n",
        "    target_lens = (ground_truth_tokens != 0).sum(-1)\n",
        "    _, input_lens = batch.sig\n",
        "    results = asr_brain.compute_forward(batch, sb.Stage.TEST)[0]\n",
        "    input_lens = torch.round(input_lens * results.shape[1]).to(torch.int)\n",
        "    break\n",
        "\n",
        "print(results.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9b4G2mMaBzsV"
      },
      "outputs": [],
      "source": [
        "def logadd(x0, x1, x2):\n",
        "\t# produces nan gradients in backward if -inf log-space zero element is used https://github.com/pytorch/pytorch/issues/31829\n",
        "\treturn torch.logsumexp(torch.stack([x0, x1, x2]), dim = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gBADwCu24x3L"
      },
      "outputs": [],
      "source": [
        "def ctc_alignment(log_probs : torch.Tensor, targets : torch.Tensor, input_lengths : torch.Tensor, target_lengths : torch.Tensor, blank : int = 0):\n",
        "    '''\n",
        "    this function computes the ctc-alignment matrix, then performs a backward-algorithm\n",
        "    pass on it to find the best state sequence (states being the transformer states here).\n",
        "    the ctc-algorithm equations can be found on the 3rd slide of the following link -\n",
        "    https://www.cse.iitb.ac.in/~pjyothi/cs753/slides/lecture15.pdf\n",
        "    keep in mind that the equations are 1-indexed while our implementation is 0-indexed\n",
        "    '''\n",
        "\n",
        "    max_input_len, batch_size, num_tokens = log_probs.shape\n",
        "    _, max_target_len = targets.shape\n",
        "\n",
        "    batch_arange = torch.arange(batch_size, device = input_lengths.device)\n",
        "\n",
        "    ##############################\n",
        "    # TODO(1) - add \"_\" (blank) token alternatively to target sequence (starting and ending with \"_\")\n",
        "    # example - \"a man saw a cat\" should become \"_a_ _m_a_n_ _s_a_w_ _a_ _c_a_t_\"\n",
        "    # (assuming that each character is one token; a token might be >1 characters long)\n",
        "    # if the original sequence had L tokens, `padded_targets` should now have 2*L+1 tokens\n",
        "   #print(targets)\n",
        "    un_ = 0\n",
        "    b = [ ]\n",
        "    for i in targets:\n",
        "        row=[un_]\n",
        "        for j in range(len(i)):\n",
        "          row.append(i[j])\n",
        "          row.append(un_)\n",
        "        b.append(row)\n",
        "\n",
        "    padded_targets = torch.tensor(b)\n",
        "    ### Complete this\n",
        "    ##############################\n",
        "    assert padded_targets.shape == (batch_size, 2 * max_target_len + 1)\n",
        "\n",
        "    # Compute positions where target[j] != target[j-2]\n",
        "    diff_labels = torch.cat([\n",
        "        torch.as_tensor([[False, False]], device = targets.device).expand(len(batch_arange), -1),\n",
        "        padded_targets[:, 2:] != padded_targets[:, :-2]\n",
        "    ], dim = 1)\n",
        "    # print(diff_labels[0,:7])\n",
        "    # Gather log-probs per input sequence index for tokens in target sequence\n",
        "    log_probs_for_targets = log_probs.gather(-1, padded_targets.unsqueeze(0).repeat(max_input_len, 1, 1))\n",
        "    assert log_probs_for_targets.shape == (max_input_len, batch_size, max_target_len * 2 + 1)\n",
        "\n",
        "    # To avoid nan grad in torch.logsumexp\n",
        "    min_element = torch.finfo(log_probs.dtype).min\n",
        "\n",
        "    # Padding is required at the start of log_alpha for vectorization\n",
        "    # Allows using torch.where for the i=j-2 step of ctc alignment computation\n",
        "    zero_padding = 2\n",
        "\n",
        "    # `log_alpha` is the ctc-probability matrix\n",
        "    # We want to deal in the logarithm-space since the probabilities can end up\n",
        "    # becoming very small and naive multiplication of small numbers can be imprecise.\n",
        "    # Note that usual-multiplication -> log-addition.\n",
        "    # Also note the utility of `logadd` function\n",
        "    log_alpha = torch.full(\n",
        "        size = (max_input_len, batch_size, zero_padding + padded_targets.shape[-1]),\n",
        "        fill_value = min_element, device = log_probs.device, dtype = log_probs.dtype\n",
        "    )\n",
        "    ##############################\n",
        "    # TODO(2) - initialization of ctc-probability matrix\n",
        "    # alpha[0] is non-zero only for first blank (\"_\") and first non-blank token\n",
        "    log_alpha[0, :, zero_padding + 0] = log_probs_for_targets[0, :, 0] ### Complete this\n",
        "    log_alpha[0, :, zero_padding + 1] = log_probs_for_targets[0, :, 1]### Complete this\n",
        "    ##############################\n",
        "\n",
        "\n",
        "    # iterate over input sequence\n",
        "    for t in range(1, max_input_len):\n",
        "        ##############################\n",
        "        # TODO(3) - compute updates using indices j, j-1, j-2; refer to slides\n",
        "        # note that updates for j-2 are conditional upon y[j-2] != y[j], use torch.where\n",
        "        # with a suitable boolean tensor defined above\n",
        "        update_j = log_alpha[t-1,:,zero_padding:] ### Complete this\n",
        "        update_j_minus_1 = log_alpha[t-1,:,zero_padding-1:-1]### Complete this\n",
        "        update_j_minus_2 = log_alpha[t-1,:,zero_padding-2:-2]*diff_labels ### Complete this\n",
        "        update_j_minus_2 = torch.where(update_j_minus_2==0,torch.tensor(min_element),update_j_minus_2)\n",
        "        ##############################\n",
        "\n",
        "\n",
        "        # multiplying with probability is equiv. to adding in the log space\n",
        "        log_alpha[t, :, zero_padding:] = logadd(\n",
        "            update_j,\n",
        "            update_j_minus_1,\n",
        "            update_j_minus_2 #######\n",
        "        ) + log_probs_for_targets[t]\n",
        "\n",
        "\n",
        "    ##############################\n",
        "    # TODO(4) - compute ctc loss terms\n",
        "    # refer to slides, the CTC(x, y) equation - loss is from time step T, tokens 2l, 2l+1\n",
        "    # T might be different for different sequences, use `input_lengths` for that\n",
        "    # l might be different for different sequences, use `target_lengths` for that\n",
        "    x =[]\n",
        "    y = []\n",
        "    for i,a in enumerate(zip(input_lengths,target_lengths)):\n",
        "      x.append(log_alpha[a[0]-1,i,zero_padding+2*a[1]-1])\n",
        "      y.append(log_alpha[a[0]-1,i,zero_padding+2*a[1]])\n",
        "\n",
        "    x = torch.tensor(x)\n",
        "    y = torch.tensor(y)\n",
        "    x = x.reshape((x.shape[0],1))\n",
        "    y = y.reshape((y.shape[0],1))\n",
        "    ctc_loss_term = torch.concatenate((x,y),axis=1)### Complete this computation. You will likely need to define\n",
        "    ### new helper variables to help with this computation.\n",
        "    ##############################\n",
        "    assert ctc_loss_term.shape == (batch_size, 2)\n",
        "\n",
        "    # `path` stores the best hidden state sequence\n",
        "    path = torch.zeros(max_input_len, batch_size, device = log_alpha.device, dtype = torch.int64)\n",
        "\n",
        "    # at timestep T, best index is 2l or 2l+1\n",
        "    path[input_lengths - 1, batch_arange] = zero_padding + 2 * target_lengths - 1 + ctc_loss_term.argmax(dim = -1)\n",
        "\n",
        "    for t, indices in reversed(list(enumerate(path))[1:]):\n",
        "        ##############################\n",
        "        # TODO(5) - compute possible previous states given the current best states in `indices`\n",
        "        # previous states could come from transition i->j where i can be j, j-1 or j-2 (if y[j] != y[j-2])\n",
        "\n",
        "        # if y[j] == y[j-2], use 0 as possible previous state; since 0 is a padding state,\n",
        "        # log_alpha will be very low and argmax below would take care of it.\n",
        "        # such tricks to handle corner-cases help vectorize code in a better fashion.\n",
        "        reshape_index = indices.reshape((indices.shape[0],1))\n",
        "        index_j_2 = diff_labels[list(range(indices.shape[0])),indices-zero_padding].reshape((indices.shape[0],1))\n",
        "        possible_previous_states = torch.concatenate(((reshape_index-2)*index_j_2,reshape_index-1,reshape_index),axis=1)\n",
        "        ##############################\n",
        "        possible_previous_states = torch.nn.functional.relu(possible_previous_states)\n",
        "        assert possible_previous_states.shape == (batch_size, 3)\n",
        "\n",
        "        # get best possible previous state\n",
        "        argmax_among_prev_states = log_alpha[t - 1, batch_arange].gather(-1, possible_previous_states).argmax(dim = -1)\n",
        "        path[t - 1] += (indices - 2 + argmax_among_prev_states).clamp(min = 0)\n",
        "\n",
        "    # color correct (input_index, target_index) cell\n",
        "    # `highlighted_best_path` is a binary (0-1) matrix\n",
        "    highlighted_best_path = torch.zeros_like(log_alpha).scatter_(\n",
        "        dim = -1,\n",
        "        index = path.unsqueeze(-1),\n",
        "        value = 1.0\n",
        "    )[..., (zero_padding + 1):]\n",
        "\n",
        "    # compute unpadded best path\n",
        "    # `highlighted_best_path` can have blank's (\"_\"), we want a mapping to the non-blank tokens only\n",
        "    # (assumed convention) when removing padding, we assign a blank token's positions to the previous token;\n",
        "    # note that the foremost blank token has already been removed above\n",
        "    unpadded_best_path = highlighted_best_path.reshape(max_input_len, batch_size, max_target_len, 2).sum(-1)\n",
        "    assert unpadded_best_path.shape == (max_input_len, batch_size, max_target_len)\n",
        "\n",
        "    return unpadded_best_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KBHYK7so-G2j"
      },
      "outputs": [],
      "source": [
        "#Computing CTC alignments using the model results\n",
        "results_in = results.cpu().transpose(0,1)\n",
        "alignment = ctc_alignment(results_in, ground_truth_tokens, input_lens, target_lens, blank = 0)\n",
        "alignment = alignment[:,0, :target_lens[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wGXi20uxgO9N"
      },
      "outputs": [],
      "source": [
        "# Tokenizer is an object of the SentencePieceProcessor class,\n",
        "# Create an array of all the tokens of tokenizer, arranged according to their token IDs\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Get the total number of tokens in the model\n",
        "def array_of_all_tokens(tokenizer):\n",
        "  num_tokens = tokenizer.get_piece_size()\n",
        "\n",
        "  all_tokens_and_ids = [(tokenizer.id_to_piece(token_id), token_id) for token_id in range(num_tokens)]\n",
        "  all_token=[]\n",
        "  for token, token_id in all_tokens_and_ids:\n",
        "      all_token.append(token)\n",
        "  all_tokens=np.array(all_token)\n",
        "  return all_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "eLwAC5qeYm2M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd6bd65c-da85-47ec-c211-157b032fa6bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numexpr.utils - NumExpr defaulting to 2 threads.\n"
          ]
        }
      ],
      "source": [
        "############################\n",
        "# TODO(6) -  Plot the alignments of the output tokens of the model and the input audio signal\n",
        "# For this, use the ctc_alignments function and the array of all tokens previously created\n",
        "############################\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "def plotting(test_data,all_tokens):\n",
        "  # Returns nothing, displays plot (save as image CTCalignments.png)\n",
        "  # Uses alignment computed two cells back\n",
        "  # Refer to the provided image in the assignment pdf for reference\n",
        "\n",
        "  a = test_data[0][\"tokens\"]\n",
        "  b = all_tokens[a]\n",
        "  ax = sns.heatmap(alignment.T,yticklabels=b)\n",
        "  ax.set_xlabel('Input Audio Timestep')\n",
        "  ax.set_ylabel('Output Tokens')\n",
        "  plt.savefig('CTCalignments.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GM0TUMWkDIeD"
      },
      "outputs": [],
      "source": [
        "def print_gt(tokenizer,ground_truth_tokens):\n",
        "  print(ground_truth_tokens[0])\n",
        "  print(tokenizer.decode([int(x) for x in ground_truth_tokens[0]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OaY7zpB9ZGM4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "outputId": "990e57b5-aec3-4cb6-d6de-912c0ce6b0de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([120,   7,  82,   4,   9,   1,  35,  47,  82,  57,  29,  26,  22,  90,\n",
            "         62,  16, 110,  11,  12, 145,   2,   1,  12,  50, 127,  31,   8,  69,\n",
            "          8,  24,  26, 112,  29,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0])\n",
            "ALL THE TIME HE WAS TALKING TO ME HIS ANGRY LITTLE EYES WERE FOLLOWING LAKE ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "all_tokens=array_of_all_tokens(tokenizer)\n",
        "plotting(test_data,all_tokens)\n",
        "print_gt(tokenizer,ground_truth_tokens)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}