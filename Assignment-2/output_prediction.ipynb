{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0,1,2,3,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/ug21/anshulv/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-11 17:30:48.504728: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-11 17:30:48.542210: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-11 17:30:48.542241: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-11 17:30:48.542258: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-11 17:30:48.549452: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-11 17:30:49.204629: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union, Optional,Tuple\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import WhisperForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "import whisper\n",
    "import tqdm\n",
    "model = whisper.load_model(\"whisper-small-finetuned.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import traceback\n",
    "import warnings\n",
    "\n",
    "from whisper.audio import (\n",
    "    FRAMES_PER_SECOND,\n",
    "    HOP_LENGTH,\n",
    "    N_FRAMES,\n",
    "    N_SAMPLES,\n",
    "    SAMPLE_RATE,\n",
    "    log_mel_spectrogram,\n",
    "    pad_or_trim,\n",
    ")\n",
    "from whisper.tokenizer import LANGUAGES, TO_LANGUAGE_CODE\n",
    "from whisper.utils import (\n",
    "    exact_div,\n",
    "    format_timestamp,\n",
    "    get_writer,\n",
    "    make_safe,\n",
    "    optional_float,\n",
    "    optional_int,\n",
    "    str2bool,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json files for test split\n",
    "data_files = {\n",
    "    \"test_blind\": \"dataset/CodeSwitched_Data/test_blind.json\"\n",
    "}\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "# Update the audio paths to include appropriate folder-name\n",
    "def prepend_folder_name(row):\n",
    "    row[\"audio\"] = 'dataset/CodeSwitched_Data/' + row[\"audio\"]\n",
    "    return row\n",
    "for key in dataset:\n",
    "    dataset[key] = dataset[key].map(prepend_folder_name)\n",
    "\n",
    "# Cast columns to appropriate features\n",
    "features = datasets.Features(\n",
    "    {\n",
    "        \"id\": datasets.Value(\"string\"),\n",
    "        \"audio\": datasets.Audio(sampling_rate=16000),\n",
    "    }\n",
    ")\n",
    "dataset = dataset.map(features.encode_example, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load necessary processors\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Hindi\", task=\"transcribe\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Hindi\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    return batch\n",
    "dataset = dataset.map(prepare_dataset, num_proc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper.decoding import DecodingOptions, DecodingResult, DecodingTask, GreedyDecoder, TokenDecoder, Inference\n",
    "from dataclasses import dataclass, field, replace\n",
    "from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from whisper.audio import CHUNK_LENGTH\n",
    "from whisper.tokenizer import Tokenizer, get_tokenizer\n",
    "from whisper.utils import compression_ratio\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from whisper.model import Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(model.is_multilingual, num_languages=model.num_languages, language=\"hi\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_in_task = set([220, 50257]).union(set(range(50257, 50364)))\n",
    "tokens_in_task = set([220, 50257])\n",
    "words_in_task = set([])\n",
    "with open(\"word_dictionary.txt\", \"r\") as f:\n",
    "    words = f.readlines()\n",
    "\n",
    "for word in words:\n",
    "    tokens_in_task = tokens_in_task.union(set(tokenizer.encode(word.strip())))\n",
    "    tokens_in_task = tokens_in_task.union(set(tokenizer.encode(\" \" + word.strip())))\n",
    "    tokens_in_task = tokens_in_task.union(set(tokenizer.encode(word.strip() + \" \")))\n",
    "    tokens_in_task = tokens_in_task.union(set(tokenizer.encode(\" \" + word.strip() + \" \")))\n",
    "    words_in_task.add(word.strip())\n",
    "\n",
    "tokens_in_task = list(tokens_in_task)\n",
    "tokens_in_task.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokens_in_task.txt\", \"w\") as f:\n",
    "    for t in tokens_in_task:\n",
    "        f.write(str(t) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECODER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper.decoding import BeamSearchDecoder\n",
    "\n",
    "\"\"\"\n",
    "Custom classes to integrate constraints within beam search decoding.\n",
    "\"\"\"\n",
    "class CustomBeamSearchDecoder(BeamSearchDecoder):\n",
    "    \"\"\"\n",
    "    Updates the existing `BeamSearchDecoder` class form whisper to use constrained beam search\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        beam_size: int,\n",
    "        eot: int,\n",
    "        inference: Inference,\n",
    "        cutoff: int = 3,\n",
    "        patience: Optional[float] = None,\n",
    "    ):\n",
    "        super().__init__(beam_size, eot, inference, patience)\n",
    "        # TODO: Add any additional variables you need for constrained beam search here\n",
    "        \n",
    "    def update(\n",
    "        self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor\n",
    "    ) -> Tuple[Tensor, bool]:\n",
    "        if tokens.shape[0] % self.beam_size != 0:\n",
    "            raise ValueError(f\"{tokens.shape}[0] % {self.beam_size} != 0\")\n",
    "\n",
    "        n_audio = tokens.shape[0] // self.beam_size\n",
    "        if self.finished_sequences is None:  # for the first update\n",
    "            self.finished_sequences = [{} for _ in range(n_audio)]\n",
    "\n",
    "        logprobs = F.log_softmax(logits.float(), dim=-1)\n",
    "        next_tokens, source_indices, finished_sequences = [], [], []\n",
    "        for i in range(n_audio):\n",
    "            scores, sources, finished = {}, {}, {}\n",
    "\n",
    "            # STEP 1: calculate the cumulative log probabilities for possible candidates\n",
    "            for j in range(self.beam_size):\n",
    "                idx = i * self.beam_size + j\n",
    "                prefix = tokens[idx].tolist()\n",
    "                for logprob, token in zip(*logprobs[idx][tokens_in_task].topk(self.beam_size + 1)):\n",
    "                # for logprob, token in zip(*logprobs[idx].topk(self.beam_size + 1)):\n",
    "                    new_logprob = (sum_logprobs[idx] + logprob).item()\n",
    "                    sequence = tuple(prefix + [tokens_in_task[token.item()]])\n",
    "                    # sequence = tuple(prefix + [token.item()])\n",
    "                    scores[sequence] = new_logprob\n",
    "                    sources[sequence] = idx\n",
    "\n",
    "            # STEP 2: rank the candidates and keep the top beam_size sequences for each audio\n",
    "            saved = 0\n",
    "            for sequence in sorted(scores, key=scores.get, reverse=True):\n",
    "                if sequence[-1] == self.eot:\n",
    "                    finished[sequence] = scores[sequence]\n",
    "                else:\n",
    "                    sum_logprobs[len(next_tokens)] = scores[sequence]\n",
    "                    next_tokens.append(sequence)\n",
    "                    source_indices.append(sources[sequence])\n",
    "\n",
    "                    saved += 1\n",
    "                    if saved == self.beam_size:\n",
    "                        break\n",
    "\n",
    "            finished_sequences.append(finished)\n",
    "\n",
    "        tokens = torch.tensor(next_tokens, device=tokens.device)\n",
    "        self.inference.rearrange_kv_cache(source_indices)\n",
    "\n",
    "        # add newly finished sequences to self.finished_sequences\n",
    "        assert len(self.finished_sequences) == len(finished_sequences)\n",
    "        for previously_finished, newly_finished in zip(\n",
    "            self.finished_sequences, finished_sequences\n",
    "        ):\n",
    "            for seq in sorted(newly_finished, key=newly_finished.get, reverse=True):\n",
    "                if len(previously_finished) >= self.max_candidates:\n",
    "                    break  # the candidate list is full\n",
    "                previously_finished[seq] = newly_finished[seq]\n",
    "\n",
    "        # mark as completed if all audio has enough number of samples\n",
    "        completed = all(\n",
    "            len(sequences) >= self.max_candidates\n",
    "            for sequences in self.finished_sequences\n",
    "        )\n",
    "        return tokens, completed\n",
    "\n",
    "    def finalize(self, preceding_tokens: Tensor, sum_logprobs: Tensor):\n",
    "        # collect all finished sequences, including patience, and add unfinished ones if not enough\n",
    "        sum_logprobs = sum_logprobs.cpu()\n",
    "        for i, sequences in enumerate(self.finished_sequences):\n",
    "            if (\n",
    "                len(sequences) < self.beam_size\n",
    "            ):  # when not enough sequences are finished\n",
    "                for j in list(np.argsort(sum_logprobs[i]))[::-1]:\n",
    "                    sequence = preceding_tokens[i, j].tolist() + [self.eot]\n",
    "                    sequences[tuple(sequence)] = sum_logprobs[i][j].item()\n",
    "                    if len(sequences) >= self.beam_size:\n",
    "                        break\n",
    "\n",
    "        tokens: List[List[Tensor]] = [\n",
    "            [torch.tensor(seq) for seq in sequences.keys()]\n",
    "            for sequences in self.finished_sequences\n",
    "        ]\n",
    "        sum_logprobs: List[List[float]] = [\n",
    "            list(sequences.values()) for sequences in self.finished_sequences\n",
    "        ]\n",
    "        return tokens, sum_logprobs\n",
    "\n",
    "class CustomDecodingTask(DecodingTask):\n",
    "    \"\"\"\n",
    "    Updates the existing `DecodingTask` class form whisper to use constrained beam search\n",
    "    \"\"\"\n",
    "    def __init__(self, model: \"Whisper\", options: DecodingOptions):\n",
    "        options = replace(options, beam_size = 2 * options.beam_size if options.beam_size is not None else None)\n",
    "        super().__init__(model, options)\n",
    "        self.decoder = CustomBeamSearchDecoder(\n",
    "            self.options.beam_size, self.tokenizer.eot, self.inference, self.options.patience\n",
    "        )\n",
    "\n",
    "@torch.no_grad()\n",
    "def custom_decode(\n",
    "    model: torch.nn.Module,\n",
    "    mel: torch.Tensor,\n",
    "    options: DecodingOptions = DecodingOptions(),\n",
    "    **kwargs,\n",
    ") -> Union[DecodingResult, List[DecodingResult]]:\n",
    "    \"\"\"\n",
    "    decode function to perform constrained beam search decoding\n",
    "    \"\"\"\n",
    "    single = mel.ndim\n",
    "    if single == 2:\n",
    "        mel = mel.unsqueeze(0)\n",
    "\n",
    "    if kwargs:\n",
    "        options = replace(options, **kwargs)\n",
    "\n",
    "    result = CustomDecodingTask(model, options).run(mel)\n",
    "\n",
    "    return result[0] if single else result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_transcribe(\n",
    "    model: \"Whisper\",\n",
    "    audio: Union[str, np.ndarray, torch.Tensor],\n",
    "    *,\n",
    "    compression_ratio_threshold: Optional[float] = 2.4,\n",
    "    logprob_threshold: Optional[float] = -1.0,\n",
    "    condition_on_previous_text: bool = True,\n",
    "    prepend_punctuations: str = \"\\\"'“¿([{-\",\n",
    "    append_punctuations: str = \"\\\"'.。,，!！?？:：”)]}、\",\n",
    "    **decode_options,\n",
    "):\n",
    "    dtype = torch.float16 if decode_options.get(\"fp16\", True) else torch.float32\n",
    "    if model.device == torch.device(\"cpu\"):\n",
    "        if torch.cuda.is_available():\n",
    "            warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
    "        if dtype == torch.float16:\n",
    "            warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
    "            dtype = torch.float32\n",
    "\n",
    "    if dtype == torch.float32:\n",
    "        decode_options[\"fp16\"] = False\n",
    "\n",
    "    # Pad 30-seconds of silence to the input audio, for slicing\n",
    "    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\n",
    "    content_frames = mel.shape[-1] - N_FRAMES\n",
    "\n",
    "    language: str = decode_options[\"language\"]\n",
    "    task: str = decode_options.get(\"task\", \"transcribe\")\n",
    "    tokenizer = get_tokenizer(\n",
    "        model.is_multilingual,\n",
    "        num_languages=model.num_languages,\n",
    "        language=language,\n",
    "        task=task,\n",
    "    )\n",
    "    \n",
    "    def decode_with_fallback(segment: torch.Tensor) -> DecodingResult:\n",
    "        decode_result = None\n",
    "\n",
    "        temp = 0.0\n",
    "        kwargs = {**decode_options}\n",
    "        kwargs.pop(\"best_of\", None)\n",
    "\n",
    "        options = DecodingOptions(**kwargs, temperature=temp)\n",
    "        decode_result = custom_decode(model, segment, options)\n",
    "\n",
    "        return decode_result\n",
    "\n",
    "    seek = 0\n",
    "    input_stride = exact_div(\n",
    "        N_FRAMES, model.dims.n_audio_ctx\n",
    "    )  # mel frames per output token: 2\n",
    "    time_precision = (\n",
    "        input_stride * HOP_LENGTH / SAMPLE_RATE\n",
    "    )  # time per output token: 0.02 (seconds)\n",
    "    all_tokens = []\n",
    "    all_segments = []\n",
    "\n",
    "    prompt_reset_since = 0\n",
    "    initial_prompt_tokens = []\n",
    "\n",
    "    def new_segment(\n",
    "        *, start: float, end: float, tokens: torch.Tensor, result: DecodingResult\n",
    "    ):\n",
    "        tokens = tokens.tolist()\n",
    "        t_tokens = [token for token in tokens if ((token >= tokenizer.eot) or (token in tokens_in_task))]\n",
    "        print(set(tokens).difference(t_tokens))\n",
    "        tokens = t_tokens\n",
    "        text_tokens = [token for token in tokens if (token < tokenizer.eot) ]\n",
    "        return {\n",
    "            \"seek\": seek,\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"text\": tokenizer.decode(text_tokens),\n",
    "            \"tokens\": tokens,\n",
    "            \"temperature\": result.temperature,\n",
    "            \"avg_logprob\": result.avg_logprob,\n",
    "            \"compression_ratio\": result.compression_ratio,\n",
    "            \"no_speech_prob\": result.no_speech_prob,\n",
    "        }\n",
    "    \n",
    "    last_speech_timestamp = 0.0\n",
    "    while seek < content_frames:\n",
    "        time_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)\n",
    "        mel_segment = mel[:, seek : seek + N_FRAMES]\n",
    "        segment_size = min(N_FRAMES, content_frames - seek)\n",
    "        segment_duration = segment_size * HOP_LENGTH / SAMPLE_RATE\n",
    "        mel_segment = pad_or_trim(mel_segment, N_FRAMES).to(model.device).to(dtype)\n",
    "\n",
    "        decode_options[\"prompt\"] = all_tokens[prompt_reset_since:]\n",
    "        result: DecodingResult = decode_with_fallback(mel_segment)\n",
    "        tokens = torch.tensor(result.tokens)\n",
    "\n",
    "        previous_seek = seek\n",
    "        current_segments = []\n",
    "\n",
    "        timestamp_tokens: torch.Tensor = tokens.ge(tokenizer.timestamp_begin)\n",
    "        single_timestamp_ending = timestamp_tokens[-2:].tolist() == [False, True]\n",
    "\n",
    "        consecutive = torch.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0]\n",
    "        consecutive.add_(1)\n",
    "        if len(consecutive) > 0:\n",
    "            # if the output contains two consecutive timestamp tokens\n",
    "            slices = consecutive.tolist()\n",
    "            if single_timestamp_ending:\n",
    "                slices.append(len(tokens))\n",
    "\n",
    "            last_slice = 0\n",
    "            for current_slice in slices:\n",
    "                sliced_tokens = tokens[last_slice:current_slice]\n",
    "                start_timestamp_pos = (\n",
    "                    sliced_tokens[0].item() - tokenizer.timestamp_begin\n",
    "                )\n",
    "                end_timestamp_pos = (\n",
    "                    sliced_tokens[-1].item() - tokenizer.timestamp_begin\n",
    "                )\n",
    "                current_segments.append(\n",
    "                    new_segment(\n",
    "                        start=time_offset + start_timestamp_pos * time_precision,\n",
    "                        end=time_offset + end_timestamp_pos * time_precision,\n",
    "                        tokens=sliced_tokens,\n",
    "                        result=result,\n",
    "                    )\n",
    "                )\n",
    "                last_slice = current_slice\n",
    "\n",
    "            if single_timestamp_ending:\n",
    "                # single timestamp at the end means no speech after the last timestamp.\n",
    "                seek += segment_size\n",
    "            else:\n",
    "                # otherwise, ignore the unfinished segment and seek to the last timestamp\n",
    "                last_timestamp_pos = (\n",
    "                    tokens[last_slice - 1].item() - tokenizer.timestamp_begin\n",
    "                )\n",
    "                seek += last_timestamp_pos * input_stride\n",
    "        else:\n",
    "            duration = segment_duration\n",
    "            timestamps = tokens[timestamp_tokens.nonzero().flatten()]\n",
    "            if (\n",
    "                len(timestamps) > 0\n",
    "                and timestamps[-1].item() != tokenizer.timestamp_begin\n",
    "            ):\n",
    "                # no consecutive timestamps but it has a timestamp; use the last one.\n",
    "                last_timestamp_pos = (\n",
    "                    timestamps[-1].item() - tokenizer.timestamp_begin\n",
    "                )\n",
    "                duration = last_timestamp_pos * time_precision\n",
    "\n",
    "            current_segments.append(\n",
    "                new_segment(\n",
    "                    start=time_offset,\n",
    "                    end=time_offset + duration,\n",
    "                    tokens=tokens,\n",
    "                    result=result,\n",
    "                )\n",
    "            )\n",
    "            seek += segment_size\n",
    "\n",
    "        # if a segment is instantaneous or does not contain text, clear it\n",
    "        for i, segment in enumerate(current_segments):\n",
    "            if segment[\"start\"] == segment[\"end\"] or segment[\"text\"].strip() == \"\":\n",
    "                segment[\"text\"] = \"\"\n",
    "                segment[\"tokens\"] = []\n",
    "                segment[\"words\"] = []\n",
    "\n",
    "        all_segments.extend(\n",
    "            [\n",
    "                {\"id\": i, **segment}\n",
    "                for i, segment in enumerate(\n",
    "                    current_segments, start=len(all_segments)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        all_tokens.extend(\n",
    "            [token for segment in current_segments for token in segment[\"tokens\"]]\n",
    "        )\n",
    "\n",
    "        if not condition_on_previous_text or result.temperature > 0.5:\n",
    "              # do not feed the prompt tokens if a high temperature was used\n",
    "              prompt_reset_since = len(all_tokens)\n",
    "\n",
    "    return dict(\n",
    "        text=tokenizer.decode(all_tokens),\n",
    "        segments=all_segments,\n",
    "        language=language,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: editdistance in /users/ug21/anshulv/.local/lib/python3.9/site-packages (0.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install editdistance\n",
    "import editdistance\n",
    "\n",
    "def cer_custom(hypothesis):\n",
    "    # Convert strings to lists of characters\n",
    "    hypothesis_chars = list(hypothesis)\n",
    "    best_cer = 100000\n",
    "    best_word = None\n",
    "    for word in words_in_task:\n",
    "        reference_chars = list(word)\n",
    "        distance = editdistance.eval(hypothesis_chars, reference_chars)\n",
    "        cer = distance / len(reference_chars)\n",
    "        if(cer < best_cer):\n",
    "            best_word = word\n",
    "            best_cer = cer\n",
    "    \n",
    "    return best_cer, best_word\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    if(sentence != \"\"):\n",
    "        # Remove the + from the sentences\n",
    "        sentence = sentence[1:]\n",
    "        words = sentence.split()\n",
    "        new_words = []\n",
    "        \n",
    "        # Remove words not in given vocab and compress multiple occurences\n",
    "        for word in words:\n",
    "            if(\"assador\" in word):\n",
    "                ind = word.find(\"assador\")\n",
    "                new_words.append(word[ : ind])\n",
    "                break\n",
    "            \n",
    "            c, w = cer_custom(word)\n",
    "            if (c > 0.25):\n",
    "                continue\n",
    "            if((len(new_words) == 0 or new_words[-1] != w)):\n",
    "                new_words.append(w)\n",
    "\n",
    "        return \" \".join(new_words)\n",
    "    else:\n",
    "        return sentence            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_json_after_processing = []\n",
    "to_json_before_processing = []\n",
    "for item in tqdm.tqdm(dataset[\"test_blind\"]):\n",
    "    prediction = custom_transcribe(model=model, audio = torch.tensor(item['audio']['array'].astype(np.float32)), language=\"hi\", beam_size=4)['text']\n",
    "    print(prediction)\n",
    "    to_json_before_processing.append({\n",
    "        'ID': item['id'],\n",
    "        'TRANSCRIPT': prediction\n",
    "    })\n",
    "    \n",
    "    prediction = process_sentence(prediction)\n",
    "    \n",
    "    print(prediction)\n",
    "    to_json_after_processing.append({\n",
    "        'ID': item['id'],\n",
    "        'TRANSCRIPT': prediction\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = pd.read_csv(\"predictions_before_proc.csv\")\n",
    "\n",
    "sentences = bf[\"TRANSCRIPT\"]\n",
    "processed = [] \n",
    "for sentence in sentences:\n",
    "    processed.append(process_sentence(str(sentence)))\n",
    "\n",
    "bf[\"TRANSCRIPT\"] = processed\n",
    "# bf.to_csv(\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf.to_csv(\"predictions.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv_file_path_after_processing = \"predictions.csv\"\n",
    "csv_file_path_before_processing = \"predictions_before_proc.csv\"\n",
    "\n",
    "x = pd.DataFrame(to_json_before_processing)\n",
    "x.to_csv(csv_file_path_before_processing, index = False)\n",
    "\n",
    "x = pd.DataFrame(to_json_after_processing)\n",
    "x.to_csv(csv_file_path_after_processing, index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
